---
title: "First Try Replication"
author: "Nikolina Filiposki but code from previous authors (partly)"
date: "2024-04-07"
output:
  html_document:
    toc: true
    toc_float: true
    css: css/lab.css
  pdf_document:
    toc: yes
header-includes:
   - \usepackage[default]{sourcesanspro}
   - \usepackage[T1]{fontenc}
mainfont: SourceSansPro
---

```{r, setup, include = FALSE }
knitr::opts_chunk$set(echo = TRUE)

p_needed <-
  c("viridis", 
    "knitr", 
    "MASS", 
    "pROC", 
    "tidyverse",
    "stargazer",
    "haven",
    "estimatr",
    "dplyr",
    "fixest",
    "ggplot2",
    "tidyr",
    "lme4",
    "brms", 
    "future")
    #"modelsummery") # not running yet, idk why 


packages <- rownames(installed.packages())
p_to_install <- p_needed[!(p_needed %in% packages)]

if (length(p_to_install) > 0) {
  install.packages(p_to_install)
}
sapply(p_needed, require, character.only = TRUE)

# This is an option for stargazer tables
#stargazer_opt <- ifelse(knitr::is_html_output(), "html")
# "latex"
```



```{r, load multilevel}
load("raw-data/multilevel_1-5-22.Rdata")
```



```{r, data wrangeling part }
# Load the Data
load("cses_imd_r/cses_imd.RData")

# Just in case backup
cses <- cses_imd

# Special case: DEU_2002 (telephone and mail-back combined here)
cses <- cses %>%
  mutate(IMD1004 = ifelse(IMD1004 %in% c("DEU12002", "DEU22002"), "DEU_2002", IMD1004))

# Special case: GRC_2015 (two time points Jan 2015 and Sep 2015 combined here)
cses <- cses %>%
  mutate(IMD1004 = ifelse(IMD1004 %in% c("GRC12015", "GRC22015"), "GRC_2015", IMD1004))



# Select variables of interest
selected_vars <- select(cses, 
                        starts_with("IMD1006_"),
                        starts_with("IMD1005"), 
                        starts_with("IMD1004"),
                        starts_with("IMD3008_"),
                        starts_with("IMD1008_"),
                        starts_with("IMD5000_"),
                        starts_with("IMD5001_"), 
                        starts_with("IMD5003_"), 
                        starts_with("IMD5005_"))

# Choose country and year of interest (case selection like Adams et al. 2022)

# Generate a vector of countries
countries <- c("AUS", "AUT", "CAN", "DNK", "FIN", "FRA", "DEU", "GBR", "GRC", "ISL",
               "IRL", "ISR", "NLD", "NZL", "NOR", "PRT", "ESP", "SWE", "CHE", "USA")

# Generate a vector of years
years <- 1996:2017

# Create an empty vector to store combinations
cy_interest <- c()

# Loop through each combination of country and year
for (country in countries) {
  for (year in years) {
    # Check if the combination exists in  dataset
    if (any(grepl(paste0(country, "_", year), selected_vars$IMD1004))) {
      # If it exists, add it to the vector
      cy_interest <- c(cy_interest, paste(country, year, sep = "_"))
    }
  }
}

# Check 
print(cy_interest)

# Use filter() to select rows where IMD1004 is one of the countries in years of interest
selected_dataset <- filter(selected_vars, IMD1004 %in% cy_interest)

 
 rm(cses_imd)
 rm(cses)
 rm(selected_vars)

```

```{r, filter like variable }
# Filter out rows with values 96, 97, and 98 from variables IMD3008_A to IMD3008_I
selected_d <- selected_dataset %>%
  filter_at(vars(starts_with("IMD3008_")), all_vars(!. %in% c(96, 97, 98)))
```



```{r, modify vote share}
# Divide variables by 100
selected_d[, c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
         "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")] <- 
  selected_d[, c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
           "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")] / 100


# Divide variables by 100
selected_d[, c("IMD5005_A", "IMD5005_B", "IMD5005_C", "IMD5005_D", 
         "IMD5005_E", "IMD5005_F", "IMD5005_G", "IMD5005_H", "IMD5005_I")] <- 
  selected_d[, c("IMD5005_A", "IMD5005_B", "IMD5005_C", "IMD5005_D", 
           "IMD5005_E", "IMD5005_F", "IMD5005_G", "IMD5005_H", "IMD5005_I")] / 100

rm(selected_dataset)
```



 Wagner (2021)


$$
\begin{equation}
Spread_i = \sqrt{\sum_{p=1}^{P} v_p \times (like_{ip} - like_i)^2}
\end{equation}
$$


where:
- \(Spread_i\) is the Weighted Affective Polarization score for respondent \(i\),
- \(P\) is the total number of parties,
- \(v_p\) is the vote share of party \(p\) (measured as a proportion with a range from 0 to 1),
- \(like_{ip}\) is the preference score given by respondent \(i\) for party \(p\), and
- \(like_i\) is the weighted average party affect for respondent \(i\).

$$
like_i = \sum_{p=1}^{P} (v_p \times like_{ip})
$$




```{r, spread like Wagner}
calculate_weighted_AP <- function(data) {
  # List of party preference variables (IMD3008_A to IMD3008_I)
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  # List of vote share variables (IMD5001_A to IMD5001_I)
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  # Initialize a vector to store weighted AP scores for each respondent
  weighted_AP_scores <- numeric(nrow(data))
  
  # Loop through each respondent
  for (i in 1:nrow(data)) {
    # Get party preference and vote share data for the current respondent
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Exclude missing values (99)
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    
    # Calculate the weighted average party affect for the current respondent
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the spread for the current respondent
    spread_i <- sqrt(sum((party_pref[valid_ratings] - likei)^2 * vote_share[valid_ratings]))
    
    # Store the weighted AP score for the current respondent
    weighted_AP_scores[i] <- spread_i
  }
  
  # Return the weighted AP scores for each respondent
  return(weighted_AP_scores)
}


weighted_AP_scores <- calculate_weighted_AP(selected_d)
print(weighted_AP_scores)

```



Need that score for dyadic data (own approach from Wagner formula): 

$$
\text{Spread}_{ip} = \sqrt{v_p \times (\text{like}_{ip} - \bar{\text{like}}_i)^2}

$$
+ $Spread_ip$ represents the affective polarization score for respondent $i$ towards party $p$.
+ $v_p$ is the vote share of party $p$ (measured as a percentage with a range from 0 to 1).
+ $like_ip$ is the like-dislike score assigned by respondent $i$ for party $p$.
+ $likeË‰_i$ is the average like-dislike score assigned by respondent $i$ across all parties.

$$
like_i = \sum_{p=1}^{P} (v_p \times like_{ip})
$$



Make several data sets: 

# 1 For all country-year where vote share and preference fits 

```{r create data set for where vote share and preference fits}
cases_full <- c("AUS_1996",
                 "AUS_2004",
                 "AUS_2007",
                 "AUS_2013",
                 "AUT_2008", 
                 "AUT_2013",
                 "AUT_2017",
                 "CAN_1997",
                 "CAN_2004",
                 "CAN_2008",
                 "CAN_2011",
                 "CAN_2015",
                 "DNK_1998",
                 "DNK_2001",
                 "DNK_2007",
                 "FIN_2003",
                 "FIN_2007",
                 "FIN_2011",
                 "FRA_2007",
                 "DEU_1998",
                 "DEU_2009", 
                 "DEU_2013",
                 "DEU_2017", 
                 "GBR_1997", 
                 "GBR_2005",
                 "GRC_2009", 
                 "GRC_2012",
                 "GRC_2015", 
                 "ISL_1999", 
                 "ISL_2003",
                 "ISL_2007",
                 "ISL_2009",
                 "ISL_2013",
                 "IRL_2002",
                 "IRL_2007",
                 "IRL_2011",
                 "IRL_2016",
                 "ISR_1996",
                 "ISR_2003",
                 "ISR_2006",
                 "ISR_2013",
                 "NLD_2002",
                 "NLD_2006",
                 "NLD_2010",
                 "NZL_1996",
                 "NZL_2002",
                 "NZL_2008",
                 "NZL_2011",
                 "NZL_2014",
                 "NOR_1997",
                 "NOR_2001",
                 "NOR_2005",
                 "NOR_2009",
                 "NOR_2013",
                 "PRT_2002",
                 "PRT_2005",
                 "PRT_2009",
                 "ESP_1996",
                 "ESP_2000",
                 "ESP_2004",
                 "SWE_1998",
                 "SWE_2002",
                 "SWE_2014",
                 "CHE_1999",
                 "CHE_2003",
                 "CHE_2007",
                 "CHE_2011",
                 "USA_1996",
                 "USA_2008",
                 "USA_2012",
                 "USA_2016"
           )

Full_selected_d <- filter(selected_d, IMD1004 %in% cases_full)
```

# 2 For France 

```{r create data set for two cases from France}
cases_fra <- c("FRA_2002", "FRA_2012")

FRA_selected_d <- filter(selected_d, IMD1004 %in% cases_fra)
```

# 3 For all country_year where vote share and preference does not fit
```{r create data set where vote share and preference do not fit}
#cases_fail <- c("DEU_2002", "DEU_2005", "NLD_1998", "PRT_2015", "SWE_2006", "USA_2004")
# Fail_selected_d <- filter(selected_d, IMD1004 %in% cases_fail)

DEU_2002 <- c("DEU_2002")
DEU_2005 <- c("DEU_2005")
NLD_1998 <- c("NLD_1998")
PRT_2015 <- c("PRT_2015")
SWE_2006 <- c("SWE_2006")
USA_2004 <- c("USA_2004")
ESP_2008 <- c("ESP_2008")
GBR_2015 <- c("GBR_2015")

# Set preference columns to missing where vote share is missing

# DEU_2002
DEU_2002_d <- filter(selected_d, IMD1004 %in% DEU_2002)
# G and H
DEU_2002_d <- DEU_2002_d %>%
  mutate(IMD3008_G = 99, IMD3008_H = 99)

# DEU_2005
DEU_2005_d <- filter(selected_d, IMD1004 %in% DEU_2005)
# G 
DEU_2005_d <- DEU_2005_d %>%
  mutate(IMD3008_G = 99)

# NLD_1998 
NLD_1998_d <- filter(selected_d, IMD1004 %in% NLD_1998)
# G
NLD_1998_d <- NLD_1998_d %>%
  mutate(IMD3008_G = 99)

# PRT_2015
PRT_2015_d <- filter(selected_d, IMD1004 %in% PRT_2015)
# H
PRT_2015_d <- PRT_2015_d %>%
  mutate(IMD3008_H = 99)

# SWE_2006
SWE_2006_d <- filter(selected_d, IMD1004 %in% SWE_2006)
# I
SWE_2006_d <- SWE_2006_d %>%
  mutate(IMD3008_I = 99)

# USA_2004
USA_2004_d <- filter(selected_d, IMD1004 %in% USA_2004)
# C
USA_2004_d <- USA_2004_d %>%
  mutate(IMD3008_C = 99)

# ESP_2008
ESP_2008_d <- filter(selected_d, IMD1004 %in% ESP_2008)
# G and H
ESP_2008_d <- ESP_2008_d %>%
  mutate(IMD3008_G = 99, IMD3008_H = 99)

# GBR_2015
GBR_2015_d <- filter(selected_d, IMD1004 %in% GBR_2015)
# E and G
GBR_2015_d <- GBR_2015_d %>% 
  mutate(IMD3008_E = 99, IMD3008_G = 99)

# Combine data sets 
combined_df <- bind_rows(DEU_2002_d, DEU_2005_d, NLD_1998_d, PRT_2015_d, 
                         SWE_2006_d, USA_2004_d, ESP_2008_d, GBR_2015_d)

rm(DEU_2002_d)
rm(DEU_2005_d)
rm(NLD_1998_d)
rm(PRT_2015_d)
rm(SWE_2006_d)
rm(USA_2004_d)
rm(ESP_2008_d)
rm(GBR_2015_d)

```


```{r, AP with ID correct one}
calculate_affective_polarization_party <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  
  return(affective_polarization_scores)
}

AP_Full_scores <- calculate_affective_polarization_party(Full_selected_d, Full_selected_d$IMD1005)
print(AP_Full_scores)


# Convert matrix to data frame
#AP_party_df <- as.data.frame(affective_polarization_party_scores)

# Add row names as a column
#AP_party_df$Respondent <- rownames(AP_party_df)


#AP_party_df <- AP_party_df %>% 
#  rename(
#    ID = V1, 
#    A = V2,
#    B = V3,
#    C = V4,
#    D = V5,
#    E = V6,
#    F = V7,
#    G = V8,
#    H = V9,
#    I = V10
#  )


# Reshape data frame into long format
#AP_party_long <- pivot_longer(AP_party_df, cols = -ID, names_to = "party_to", values_to = "WAP")

```


```{r, AP for two cases of France}
calculate_AP_FRA <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5005_A", "IMD5005_B", "IMD5005_C", "IMD5005_D", 
                       "IMD5005_E", "IMD5005_F", "IMD5005_G", "IMD5005_H", "IMD5005_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
#  colnames(affective_polarization_scores) <- c("ID", paste0("Affective_Polarization_", LETTERS[1:length(party_pref_vars)]))
  #colnames(affective_polarization_scores)[1] <- "ID"  # Rename the first column only
  
  return(affective_polarization_scores)
}

AP_FRA_scores <- calculate_AP_FRA(FRA_selected_d, FRA_selected_d$IMD1005)
print(AP_FRA_scores)

# partei of null setzten 
```


```{r, AP special six }
calculate_affective_polarization_party <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  
  return(affective_polarization_scores)
}

AP_six_spe <- calculate_affective_polarization_party(combined_df, combined_df$IMD1005)
print(AP_six_spe)
```


```{r, combine matricies}

AP_all_scores <- rbind(AP_Full_scores, AP_FRA_scores, AP_six_spe)# AP_Fail_scores #  AP_FRA_scores

# Convert matrix to data frame
AP_party_df <- as.data.frame(AP_all_scores)

# Add row names as a column
#AP_party_df$Respondent <- rownames(AP_party_df)


AP_party_df <- AP_party_df %>% 
  rename(
    ID = V1, 
    A = V2,
    B = V3,
    C = V4,
    D = V5,
    E = V6,
    F = V7,
    G = V8,
    H = V9,
    I = V10
  )


# Reshape data frame into long format
AP_party_long <- pivot_longer(AP_party_df, cols = -ID, names_to = "party_to", values_to = "WAP")

rm(AP_six_spe)
rm(AP_FRA_scores)
rm(AP_Full_scores)

rm(combined_df)
rm(FRA_selected_d)
rm(Full_selected_d)
```


```{r, merge AP_party_long mit multilevel_data }

# Perform the merge
merged_data <- merge(AP_party_long, multilevel_data, by = c("party_to", "ID"))

```


```{r, data wrangeling merged_data}
## Removing smaller parties

merged_data <- merged_data %>%
  filter(to_prior_seats >= 4) %>%
  select(rile_distance_s, prior_coalition, prior_opposition, econ_distance_s, society_distance_s,
         year, country, party_dislike, party_like, cntryyr, to_pfeml, to_prior_seats, to_mp_number, WAP) %>%
  na.omit()

merged_data$to_pfeml <- as.numeric(merged_data$to_pfeml)
merged_data$WAP <- as.numeric(merged_data$WAP)


# check
sum(is.na(merged_data$to_pfeml))

rm(AP_all_scores)
rm(AP_party_df)
rm(AP_party_long)
rm(multilevel_data)
rm(selected_d)

```
```{r}
hist(merged_data[merged_data$cntryyr == "Great Britain2015", "to_pfeml"])

#hist(merged_data[merged_data$cntryyr== "Great Britain2015", "WAP"] )


m <- merged_data[merged_data$cntryyr == "Great Britain2015",  c("cntryyr", "WAP")]

#hist(merged_data[merged_data$cntryyr, "WAP"])

m <- merged_data[, c("country", "year", "to_pfeml", "WAP")]
```


```{r}
hist(merged_data[merged_data$cntryyr == "Australia1996", "WAP"])
```
 


```{r, perform first multilevel analysis}

# Adams et al. 2023 
#table1.1 <-lm(party_like ~ to_pfeml + as.factor(cntryyr), data = merged_data)
#summary(table1.1)


#table1 <- lm(WAP ~ to_pfeml + as.factor(cntryyr), data = merged_data)
#summary(table1)


mlm_1 <- lmer(WAP ~ to_pfeml +
                (1 + to_pfeml | cntryyr),
              data = merged_data)

summary(mlm_1)

ranef(mlm_1)




#table2 <- lm(WAP ~ to_pfeml + as.factor(country), data = merged_data)
#summary(table2)

#table3 <- lm(WAP ~ to_pfeml + as.factor(year), data = merged_data)
#summary(table3)


#load("raw-data/dyadic_data_1-4-22.Rdata")
```


```{r}

# Fit the model
mlm_1 <- lmer(WAP ~ to_pfeml + (1 + to_pfeml | cntryyr), data = merged_data)

# Extract variance components
var_comp <- as.data.frame(VarCorr(mlm_1))

# Extract the variance of the intercept for 'cntryyr'
var_cntryyr <- var_comp[var_comp$grp == "cntryyr" & var_comp$var1 == "(Intercept)" & is.na(var_comp$var2), "vcov"]

# Extract residual variance
var_residual <- attr(VarCorr(mlm_1), "sc")^2

# Calculate ICC
ICC <- var_cntryyr / (var_cntryyr + var_residual)
print(ICC)
```

Truncated model: 

$$
\text{WAP}_{ij} \mid 0 \leq \text{WAP}_{ij} \leq 4 \sim \mathcal{N}(\mu_{ij}, \sigma)
$$

$$
\mu_{ij} = \beta_0 + \beta_1 \text{to_pfeml}_{ij} + (u_{0j} + u_{1j} \text{to_pfeml}_{ij})
$$

$$
u_{0j} \sim \mathcal{N}(0, \tau_0^2), \quad u_{1j} \sim \mathcal{N}(0, \tau_1^2), \quad \text{Cov}(u_{0j}, u_{1j}) = \tau_{01}
$$


where $j$ indexes the groups and $i$  indexes the individual observations.


```{r, full model without priors}
#install.packages("Brobdingnag")
#install.packages("brms", dependencies = TRUE)
#library(brms)

plan(multisession, workers = 4)

# Define the model with truncation bounds
truncated_model <- brm(
  bf(WAP | trunc(lb = 0, ub = 4) ~ to_pfeml + (1 + to_pfeml | cntryyr)),
  data = merged_data,
  family = gaussian(),  # Assuming WAP is normally distributed
  chains = 4,          # Number of MCMC chains
  iter = 2000,
  warmup = 1000,# Number of iterations per chain
  cores = 4, 
  save_model = "full_model.rds"
)


# In case of interruptions, reload the saved model
# truncated_model_full <- readRDS("full_model.rds")

# Summarize the model
summary(truncated_model)

save(truncated_model, file = "truncated_model.RData")
#load("truncated_model.RData")

summary(truncated_model)$rhat

summary(truncated_model)

pp_check(truncated_model)

residuals <- residuals(truncated_model)
plot(residuals)

plot(truncated_model)
```

```{r, brm with priors}
priors <- c(
  prior(normal(0, 1), class = "b"),          # Coefficients
  prior(cauchy(0, 1), class = "sd"),         # Standard deviations
  prior(lkj(2), class = "cor")               # Correlations
)


truncated_model_with_priors <- brm(
  bf(log_WAP | trunc(lb = log1p(0), ub = log1p(4)) ~ to_pfeml + (1 + to_pfeml | cntryyr)),
  data = merged_data,
  family = gaussian(),
  chains = 4,
  iter = 4000,
  warmup = 1000,
  cores = 4,
  save_model = "full_model_transformed.rds",
  prior = priors
)


m <- merged_data[merged_data$WAP >= 0 & merged_data$WAP <= 1, c("country", "year", "WAP", "to_pfeml")]
n <- merged_data[merged_data$WAP >= 1 & merged_data$WAP <= 2, c("country", "year", "WAP", "to_pfeml")]
o <- merged_data[merged_data$WAP >= 2 & merged_data$WAP <= 3, c("country", "year", "WAP", "to_pfeml")]
p <- merged_data[merged_data$WAP >= 3 & merged_data$WAP <= 4, c("country", "year", "WAP", "to_pfeml")]


merged_data$log_WAP <- log1p(merged_data$WAP)

q <- merged_data[, c("country", "year", "WAP_transformed", "to_pfeml")]

hist(log1p(merged_data$log_WAP), breaks=50, main="Transformed WAP")


```

```{r, test truncated model with subset and varying intercept}
# Use a random subset of 10% of the data for testing
set.seed(123)
# Assuming merged_data is your full dataset
sampled_data <- merged_data %>%
  group_by(cntryyr) %>%
  sample_frac(0.1) %>%  # Randomly sample 10% of data within each group
  ungroup()

# Fit the model on the subset
truncated_model_subset <- brm(
  bf(WAP | trunc(lb = 0, ub = 4) ~ to_pfeml + (1 | cntryyr)),
  data = sampled_data,
  family = gaussian(),
  chains = 2,
  iter = 2000, # lower for quicker testing
  cores = 4
)

# Plot trace plots to inspect chains visually
truncated_model_subset <- plot(truncated_model_subset)
save(truncated_model_subset, file = "truncated_model_subset.png")

# Plot autocorrelation
mcmc_plot_subset <- mcmc_plot(truncated_model_subset, type = "acf")
save(mcmc_plot_subset, file = "mcmc_plot_subset.png")

# summary
summary_truncated_model_subset <- summary(truncated_model_subset)
#save(summary_truncated_model_subset, file = "summary_truncated_model_subset.docx")

save(truncated_model_subset, file = "truncated_model_subset.RData")
#load("truncated_model_subset.RData")

# Posterior Predictive Checks: To ensure the model fits the data well, 
# perform posterior predictive checks
pp_check_subset <- pp_check(truncated_model_subset)
save(pp_check_subset, file = "pp_check_subset.png")

```


```{r}
#library(dplyr)
#library(brms)
#library(MASS)  


# Filter data to include only the countries with the highest and lowest to_pfeml share
highest_lowest_countries <- merged_data %>%
  group_by(cntryyr) %>%
  summarise(avg_to_pfeml = mean(to_pfeml, na.rm = TRUE)) %>%
  top_n(2, avg_to_pfeml) %>%
  pull(cntryyr)

filtered_data <- merged_data %>%
  filter(cntryyr %in% highest_lowest_countries)

# Fit the model (if not already fitted)
# truncated_model <- brm(
#   bf(WAP | trunc(lb = 0, ub = 4) ~ to_pfeml + (1 + to_pfeml | cntryyr)),
#   data = filtered_data,
#   family = gaussian(),
#   chains = 4,
#   iter = 2000
# )

# Step 1: Get the regression coefficients (fixed effects)
beta_hat <- fixef(truncated_model)

# Step 2.1: Get the variance-covariance matrix
V_hat <- vcov(truncated_model)

# Step 2.2: Draw samples from the multivariate normal distribution
nsim <- 10000
S <- mvrnorm(nsim, beta_hat, V_hat)

# Step 3: Add random effects
random_effects <- ranef(truncated_model)$cntryyr[highest_lowest_countries, , drop = FALSE]
S[, "(Intercept)"] <- S[, "(Intercept)"] + random_effects[, "(Intercept)"]
S[, "to_pfeml"] <- S[, "to_pfeml"] + random_effects[, "to_pfeml"]

# Step 4: Choose interesting covariate values
min_to_pfeml <- min(filtered_data$to_pfeml, na.rm = TRUE)
max_to_pfeml <- max(filtered_data$to_pfeml, na.rm = TRUE)
to_pfeml_seq <- seq(min_to_pfeml, max_to_pfeml, length.out = 100)
scenario <- cbind(1, to_pfeml_seq)

# Step 5: Calculate Quantities of Interest - Expected Values
EV <- S %*% t(scenario)

# Step 6: Summarize
ev_mean <- apply(EV, 2, mean)
ev_ci <- apply(EV, 2, quantile, c(0.025, 0.975))

```



```{r, qq-plot}
qqnorm(resid(mlm_1)) ; qqline(resid(mlm_1), col="red")
```


```{r, heteroskedastiscity}
ggplot(data = merged_data, aes(x = predict(mlm_1), y = cbind(resid(mlm_1)))) + 
  geom_point(size = 0.5, col = "red", alpha = 0.3) +
  geom_abline(slope = 0,
              intercept = 0,
              col = "gray") + 
  labs(x = "Fitted Values", y = "Residuals", 
       title = "Residuals vs Fitted", 
       subtitle = "Homogeneity") + 
  coord_fixed() 




# Fit the multilevel model
mlm_1 <- lmer(WAP ~ to_pfeml + (1 + to_pfeml | cntryyr), data = merged_data)

# Extract fitted values and residuals
merged_data$fitted_values <- fitted(mlm_1)
merged_data$residuals <- resid(mlm_1)

# Plot residuals vs. fitted values within each group
ggplot(merged_data, aes(x = fitted_values, y = residuals)) +
  geom_point(size = 2, color = "red", alpha = 0.5) +
  #facet_wrap(~ cntryyr) +  # Facet by group
  geom_hline(yintercept = 0, color = "gray", linetype = "dashed") +
  labs(x = "Fitted Values", y = "Residuals",
       title = "Residuals vs Fitted Values by Group",
       subtitle = "Assessing Within-Group Heteroscedasticity") +
  theme_minimal()

```


```{r, residuals mlm_1}
hist(resid(mlm_1), main = "Histogram of Residuals (normality)", 
     xlab = "Residuals", col = "red")
```


```{r, simulation (working)}
nsim <- 10000

# Step 1: Get the regression coefficients
#   NOTE: We are only using fixed effects
beta_hat <- fixef(mlm_1)

# Step 2: Generate sampling distribution

# Step 2.1: Get the variance-covariance matrix.
V_hat <-  vcov(mlm_1)

# Step 2.2: Draw from the multivariate normal distribution.
S <- mvrnorm(nsim, beta_hat, V_hat)

# Additional Step 2.3: Add random effects

# varying intercept
S[, "(Intercept)"] <- 
  S[, "(Intercept)"] + 
  ranef(mlm_1)$country["Spain", "(Intercept)"]
# varying slope
S[, "to_pfeml"] <- 
  S[, "to_pfeml"] + 
  ranef(mlm_1)$country["Spain", "to_pfeml"]



# Step 3: Choose interesting covariate values. 
# Make sure the matrix multiplication also works for single scenarios
min_to_pfeml <- min(merged_data$to_pfeml[merged_data$country == "Spain"], na.rm = T)
max_to_pfeml <- max(merged_data$to_pfeml[merged_data$country == "Spain"], na.rm = T)
to_pfeml_seq <- seq(min_to_pfeml, max_to_pfeml,
                     length.out = 100)

scenario <- cbind(1, to_pfeml_seq)

# Step 4: Calculate Quantities of Interest - 
# Expected Values
EV <- S %*% t(scenario)

# Step 5: Summarize
ev_mean <- apply(EV, 2, mean)
ev_ci <- apply(EV, 2, quantile, c(0.025, 0.975))
```

```{r, plot simulation}
plot(x = to_pfeml_seq,
     y = ev_mean,
     ylim = c(0.1, 1),
     xlim = c(0.0, 0.5),
     type = "l",
     lwd = 1.5,
     main = "Female share and WAP in Spain",
     font.main = 1,
     ylab = "Expected value of WAP",
     xlab = "Average female share",
     las = 1)
lines(x = to_pfeml_seq,
      y = ev_ci["2.5%",],
      lty = "dashed",
      lwd = 1.5)
lines(x = to_pfeml_seq,
      y = ev_ci["97.5%",],
      lty = "dashed",
      lwd = 1.5)
```















