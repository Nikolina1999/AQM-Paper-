---
title: "First Try Replication"
author: "Nikolina Filiposki"
date: "2024-04-07"
output:
  html_document:
    toc: true
    toc_float: true
    css: css/lab.css
  pdf_document:
    toc: yes
header-includes:
   - \usepackage[default]{sourcesanspro}
   - \usepackage[T1]{fontenc}
mainfont: SourceSansPro
---

```{r, setup, include = FALSE }
knitr::opts_chunk$set(echo = TRUE)

p_needed <-
  c("viridis", 
    "knitr", 
    "MASS", 
    "pROC", 
    "tidyverse",
    "stargazer",
    "haven",
    "estimatr",
    "dplyr",
    "fixest",
    "ggplot2",
    "tidyr",
    "lme4",
    "brms", 
    "future")
    #"modelsummery") # not running yet, idk why 


packages <- rownames(installed.packages())
p_to_install <- p_needed[!(p_needed %in% packages)]

if (length(p_to_install) > 0) {
  install.packages(p_to_install)
}
sapply(p_needed, require, character.only = TRUE)

# This is an option for stargazer tables
#stargazer_opt <- ifelse(knitr::is_html_output(), "html")
# "latex"
```



```{r, load multilevel}
load("raw-data/multilevel_1-5-22.Rdata")
```



```{r, data wrangeling part }
# Load the Data
load("cses_imd_r/cses_imd.RData")

# Just in case backup
cses <- cses_imd

# Special case: DEU_2002 (telephone and mail-back combined here)
cses <- cses %>%
  mutate(IMD1004 = ifelse(IMD1004 %in% c("DEU12002", "DEU22002"), "DEU_2002", IMD1004))

# Special case: GRC_2015 (two time points Jan 2015 and Sep 2015 combined here)
cses <- cses %>%
  mutate(IMD1004 = ifelse(IMD1004 %in% c("GRC12015", "GRC22015"), "GRC_2015", IMD1004))



# Select variables of interest
selected_vars <- select(cses, 
                        starts_with("IMD1006_"),
                        starts_with("IMD1005"), 
                        starts_with("IMD1004"),
                        starts_with("IMD3008_"),
                        starts_with("IMD1008_"),
                        starts_with("IMD5000_"),
                        starts_with("IMD5001_"), 
                        starts_with("IMD5003_"), 
                        starts_with("IMD5005_"))

# Choose country and year of interest (case selection like Adams et al. 2022)

# Generate a vector of countries
countries <- c("AUS", "AUT", "CAN", "DNK", "FIN", "FRA", "DEU", "GBR", "GRC", "ISL",
               "IRL", "ISR", "NLD", "NZL", "NOR", "PRT", "ESP", "SWE", "CHE", "USA")

# Generate a vector of years
years <- 1996:2017

# Create an empty vector to store combinations
cy_interest <- c()

# Loop through each combination of country and year
for (country in countries) {
  for (year in years) {
    # Check if the combination exists in  dataset
    if (any(grepl(paste0(country, "_", year), selected_vars$IMD1004))) {
      # If it exists, add it to the vector
      cy_interest <- c(cy_interest, paste(country, year, sep = "_"))
    }
  }
}

# Check 
print(cy_interest)

# Use filter() to select rows where IMD1004 is one of the countries in years of interest
selected_dataset <- filter(selected_vars, IMD1004 %in% cy_interest)

 
 rm(cses_imd)
 rm(cses)
 rm(selected_vars)

```

```{r, filter like variable }
# Filter out rows with values 96, 97, and 98 from variables IMD3008_A to IMD3008_I
selected_d <- selected_dataset %>%
  filter_at(vars(starts_with("IMD3008_")), all_vars(!. %in% c(96, 97, 98)))
```



```{r, modify vote share}
# Divide variables by 100
selected_d[, c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
         "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")] <- 
  selected_d[, c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
           "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")] / 100


# Divide variables by 100
selected_d[, c("IMD5005_A", "IMD5005_B", "IMD5005_C", "IMD5005_D", 
         "IMD5005_E", "IMD5005_F", "IMD5005_G", "IMD5005_H", "IMD5005_I")] <- 
  selected_d[, c("IMD5005_A", "IMD5005_B", "IMD5005_C", "IMD5005_D", 
           "IMD5005_E", "IMD5005_F", "IMD5005_G", "IMD5005_H", "IMD5005_I")] / 100

rm(selected_dataset)
```



 Wagner (2021) has the following formular:


$$
\begin{equation}
Spread_i = \sqrt{\sum_{p=1}^{P} v_p \times (like_{ip} - like_i)^2}
\end{equation}
$$


where:
- \(Spread_i\) is the Weighted Affective Polarization score for respondent \(i\),
- \(P\) is the total number of parties,
- \(v_p\) is the vote share of party \(p\) (measured as a proportion with a range from 0 to 1),
- \(like_{ip}\) is the preference score given by respondent \(i\) for party \(p\), and
- \(like_i\) is the weighted average party affect for respondent \(i\).

$$
like_i = \sum_{p=1}^{P} (v_p \times like_{ip})
$$

Need that score for dyadic data (own approach from Wagner formula): 

$$
\text{Spread}_{ip} = \sqrt{v_p \times (\text{like}_{ip} - \bar{\text{like}}_i)^2}

$$
+ $Spread_ip$ represents the affective polarization score for respondent $i$ towards party $p$.
+ $v_p$ is the vote share of party $p$ (measured as a percentage with a range from 0 to 1).
+ $like_ip$ is the like-dislike score assigned by respondent $i$ for party $p$.
+ $likeË‰_i$ is the average like-dislike score assigned by respondent $i$ across all parties.

$$
like_i = \sum_{p=1}^{P} (v_p \times like_{ip})
$$



Make several data sets: 

# 1 For all country-year where vote share and preference fits 

```{r create data set for where vote share and preference fits}
cases_full <- c("AUS_1996",
                 "AUS_2004",
                 "AUS_2007",
                 "AUS_2013",
                 "AUT_2008", 
                 "AUT_2013",
                 "AUT_2017",
                 "CAN_1997",
                 "CAN_2004",
                 "CAN_2008",
                 "CAN_2011",
                 "CAN_2015",
                 "DNK_1998",
                 "DNK_2001",
                 "DNK_2007",
                 "FIN_2003",
                 "FIN_2007",
                 "FIN_2011",
                 "FRA_2007",
                 "DEU_1998",
                 "DEU_2009", 
                 "DEU_2013",
                 "DEU_2017", 
                 "GBR_1997", 
                 "GBR_2005",
                 "GRC_2009", 
                 "GRC_2012",
                 "GRC_2015", 
                 "ISL_1999", 
                 "ISL_2003",
                 "ISL_2007",
                 "ISL_2009",
                 "ISL_2013",
                 "IRL_2002",
                 "IRL_2007",
                 "IRL_2011",
                 "IRL_2016",
                 "ISR_1996",
                 "ISR_2003",
                 "ISR_2006",
                 "ISR_2013",
                 "NLD_2002",
                 "NLD_2006",
                 "NLD_2010",
                 "NZL_1996",
                 "NZL_2002",
                 "NZL_2008",
                 "NZL_2011",
                 "NZL_2014",
                 "NOR_1997",
                 "NOR_2001",
                 "NOR_2005",
                 "NOR_2009",
                 "NOR_2013",
                 "PRT_2002",
                 "PRT_2005",
                 "PRT_2009",
                 "ESP_1996",
                 "ESP_2000",
                 "ESP_2004",
                 "SWE_1998",
                 "SWE_2002",
                 "SWE_2014",
                 "CHE_1999",
                 "CHE_2003",
                 "CHE_2007",
                 "CHE_2011",
                 "USA_1996",
                 "USA_2008",
                 "USA_2012",
                 "USA_2016"
           )

Full_selected_d <- filter(selected_d, IMD1004 %in% cases_full)
```

# 2 For France 

```{r create data set for two cases from France}
cases_fra <- c("FRA_2002", "FRA_2012")

FRA_selected_d <- filter(selected_d, IMD1004 %in% cases_fra)
```

# 3 For all country_year where vote share and preference does not fit
```{r create data set where vote share and preference do not fit}
#cases_fail <- c("DEU_2002", "DEU_2005", "NLD_1998", "PRT_2015", "SWE_2006", "USA_2004")
# Fail_selected_d <- filter(selected_d, IMD1004 %in% cases_fail)

DEU_2002 <- c("DEU_2002")
DEU_2005 <- c("DEU_2005")
NLD_1998 <- c("NLD_1998")
PRT_2015 <- c("PRT_2015")
SWE_2006 <- c("SWE_2006")
USA_2004 <- c("USA_2004")
ESP_2008 <- c("ESP_2008")
GBR_2015 <- c("GBR_2015")

# Set preference columns to missing where vote share is missing

# DEU_2002
DEU_2002_d <- filter(selected_d, IMD1004 %in% DEU_2002)
# G and H
DEU_2002_d <- DEU_2002_d %>%
  mutate(IMD3008_G = 99, IMD3008_H = 99)

# DEU_2005
DEU_2005_d <- filter(selected_d, IMD1004 %in% DEU_2005)
# G 
DEU_2005_d <- DEU_2005_d %>%
  mutate(IMD3008_G = 99)

# NLD_1998 
NLD_1998_d <- filter(selected_d, IMD1004 %in% NLD_1998)
# G
NLD_1998_d <- NLD_1998_d %>%
  mutate(IMD3008_G = 99)

# PRT_2015
PRT_2015_d <- filter(selected_d, IMD1004 %in% PRT_2015)
# H
PRT_2015_d <- PRT_2015_d %>%
  mutate(IMD3008_H = 99)

# SWE_2006
SWE_2006_d <- filter(selected_d, IMD1004 %in% SWE_2006)
# I
SWE_2006_d <- SWE_2006_d %>%
  mutate(IMD3008_I = 99)

# USA_2004
USA_2004_d <- filter(selected_d, IMD1004 %in% USA_2004)
# C
USA_2004_d <- USA_2004_d %>%
  mutate(IMD3008_C = 99)

# ESP_2008
ESP_2008_d <- filter(selected_d, IMD1004 %in% ESP_2008)
# G and H
ESP_2008_d <- ESP_2008_d %>%
  mutate(IMD3008_G = 99, IMD3008_H = 99)

# GBR_2015
GBR_2015_d <- filter(selected_d, IMD1004 %in% GBR_2015)
# E and G
GBR_2015_d <- GBR_2015_d %>% 
  mutate(IMD3008_E = 99, IMD3008_G = 99)

# Combine data sets 
combined_df <- bind_rows(DEU_2002_d, DEU_2005_d, NLD_1998_d, PRT_2015_d, 
                         SWE_2006_d, USA_2004_d, ESP_2008_d, GBR_2015_d)

rm(DEU_2002_d)
rm(DEU_2005_d)
rm(NLD_1998_d)
rm(PRT_2015_d)
rm(SWE_2006_d)
rm(USA_2004_d)
rm(ESP_2008_d)
rm(GBR_2015_d)

```


```{r, AP with ID correct one}
calculate_affective_polarization_party <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  
  return(affective_polarization_scores)
}

AP_Full_scores <- calculate_affective_polarization_party(Full_selected_d, Full_selected_d$IMD1005)
print(AP_Full_scores)


# Convert matrix to data frame
#AP_party_df <- as.data.frame(affective_polarization_party_scores)

# Add row names as a column
#AP_party_df$Respondent <- rownames(AP_party_df)


#AP_party_df <- AP_party_df %>% 
#  rename(
#    ID = V1, 
#    A = V2,
#    B = V3,
#    C = V4,
#    D = V5,
#    E = V6,
#    F = V7,
#    G = V8,
#    H = V9,
#    I = V10
#  )


# Reshape data frame into long format
#AP_party_long <- pivot_longer(AP_party_df, cols = -ID, names_to = "party_to", values_to = "WAP")

```


```{r, AP for two cases of France}
calculate_AP_FRA <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5005_A", "IMD5005_B", "IMD5005_C", "IMD5005_D", 
                       "IMD5005_E", "IMD5005_F", "IMD5005_G", "IMD5005_H", "IMD5005_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
#  colnames(affective_polarization_scores) <- c("ID", paste0("Affective_Polarization_", LETTERS[1:length(party_pref_vars)]))
  #colnames(affective_polarization_scores)[1] <- "ID"  # Rename the first column only
  
  return(affective_polarization_scores)
}

AP_FRA_scores <- calculate_AP_FRA(FRA_selected_d, FRA_selected_d$IMD1005)
print(AP_FRA_scores)

# partei of null setzten 
```


```{r, AP special six }
calculate_affective_polarization_party <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  
  return(affective_polarization_scores)
}

AP_six_spe <- calculate_affective_polarization_party(combined_df, combined_df$IMD1005)
print(AP_six_spe)
```


```{r, combine matricies}

AP_all_scores <- rbind(AP_Full_scores, AP_FRA_scores, AP_six_spe)# AP_Fail_scores #  AP_FRA_scores

# Convert matrix to data frame
AP_party_df <- as.data.frame(AP_all_scores)

# Add row names as a column
#AP_party_df$Respondent <- rownames(AP_party_df)


AP_party_df <- AP_party_df %>% 
  rename(
    ID = V1, 
    A = V2,
    B = V3,
    C = V4,
    D = V5,
    E = V6,
    F = V7,
    G = V8,
    H = V9,
    I = V10
  )


# Reshape data frame into long format
AP_party_long <- pivot_longer(AP_party_df, cols = -ID, names_to = "party_to", values_to = "WAP")

rm(AP_six_spe)
rm(AP_FRA_scores)
rm(AP_Full_scores)

rm(combined_df)
rm(FRA_selected_d)
rm(Full_selected_d)
```


```{r, merge AP_party_long mit multilevel_data }

# Perform the merge
merged_data <- merge(AP_party_long, multilevel_data, by = c("party_to", "ID"))

```


```{r, data wrangeling merged_data}
## Removing smaller parties

merged_data <- merged_data %>%
  filter(to_prior_seats >= 4) %>%
  select(rile_distance_s, prior_coalition, prior_opposition, econ_distance_s, society_distance_s,
         year, country, party_dislike, party_like, cntryyr, to_pfeml, to_prior_seats, to_mp_number, WAP) %>%
  na.omit()

merged_data$to_pfeml <- as.numeric(merged_data$to_pfeml)
merged_data$WAP <- as.numeric(merged_data$WAP)


# check
sum(is.na(merged_data$to_pfeml))

# Transform WAP because skewed
merged_data$log_WAP <- log1p(merged_data$WAP)
hist(log1p(merged_data$log_WAP), breaks=50, main="Transformed WAP")

# We need RAM for this project, get rid of what we do not need anymore
rm(AP_all_scores)
rm(AP_party_df)
rm(AP_party_long)
rm(multilevel_data)
rm(selected_d)

```
```{r, chunk to test stuff}
#hist(merged_data[merged_data$cntryyr == "Great Britain2015", "to_pfeml"])

#hist(merged_data[merged_data$cntryyr== "Great Britain2015", "WAP"] )

#hist(merged_data[merged_data$cntryyr, "WAP"])

#m <- merged_data[, c("country", "year", "to_pfeml", "WAP")]

#q <- merged_data[, c("country", "year", "WAP_transformed", "to_pfeml")]
```



Truncated model: 

$$
\text{WAP}_{ij} \mid 0 \leq \text{WAP}_{ij} \leq 4 \sim \mathcal{N}(\mu_{ij}, \sigma)
$$

$$
\mu_{ij} = \beta_0 + \beta_1 \text{to_pfeml}_{ij} + (u_{0j} + u_{1j} \text{to_pfeml}_{ij})
$$

$$
u_{0j} \sim \mathcal{N}(0, \tau_0^2), \quad u_{1j} \sim \mathcal{N}(0, \tau_1^2), \quad \text{Cov}(u_{0j}, u_{1j}) = \tau_{01}
$$


where $j$ indexes the groups and $i$  indexes the individual observations.


Attention!
The following chunks need several hours to days to run because of brms()!
Please install RTools and brms() as well as Brobdingnag() to run those chunks (this will start automatically if you run the chunk and have no RTools implementation yet). Make sure that you path matches with your system variables and that you run the code on all available cores with future() package to enhance the calculation! 

You can also just reload the already previously calculated outcomes with load() from RProject and proceed as usual. 

```{r, test truncated model with subset and varying intercept}
# Use a random subset of 10% of the data for testing
set.seed(123)
# Assuming merged_data is your full dataset
sampled_data <- merged_data %>%
  group_by(cntryyr) %>%
  sample_frac(0.1) %>%  # Randomly sample 10% of data within each group
  ungroup()

# Fit the model on the subset
truncated_model_subset <- brm(
  bf(WAP | trunc(lb = 0, ub = 4) ~ to_pfeml + (1 | cntryyr)),
  data = sampled_data,
  family = gaussian(),
  chains = 2,
  iter = 2000, # lower for quicker testing
  cores = 4
)

# Plot trace plots to inspect chains visually
truncated_model_subset <- plot(truncated_model_subset)
save(truncated_model_subset, file = "truncated_model_subset.png")

# Plot autocorrelation
mcmc_plot_subset <- mcmc_plot(truncated_model_subset, type = "acf")
save(mcmc_plot_subset, file = "mcmc_plot_subset.png")

# summary
summary_truncated_model_subset <- summary(truncated_model_subset)
#save(summary_truncated_model_subset, file = "summary_truncated_model_subset.docx")

save(truncated_model_subset, file = "truncated_model_subset.RData")
#load("truncated_model_subset.RData")

# Posterior Predictive Checks: To ensure the model fits the data well, 
# perform posterior predictive checks
pp_check_subset <- pp_check(truncated_model_subset)
save(pp_check_subset, file = "pp_check_subset.png")

```

```{r, full model with default priors (did not converge because to less iter)}
#install.packages("Brobdingnag")
#install.packages("brms", dependencies = TRUE)
#library(brms)

plan(multisession, workers = 4)

# Define the model with truncation bounds
truncated_model <- brm(
  bf(WAP | trunc(lb = 0, ub = 4) ~ to_pfeml + (1 + to_pfeml | cntryyr)),
  data = merged_data,
  family = gaussian(),  # Assuming WAP is normally distributed
  chains = 4,          # Number of MCMC chains
  iter = 2000,
  warmup = 1000,# Number of iterations per chain
  cores = 4, 
  save_model = "full_model.rds"
)


# In case of interruptions, reload the saved model
# truncated_model_full <- readRDS("full_model.rds")

# Summarize the model
summary(truncated_model)

save(truncated_model, file = "truncated_model.RData")
#load("truncated_model.RData")

summary(truncated_model)$rhat

summary(truncated_model)

pp_check(truncated_model)

residuals <- residuals(truncated_model)
plot(residuals)

plot(truncated_model)
```

```{r, brm with priors (correct one)}
priors <- c(
  prior(normal(0, 1), class = "b"),          # Coefficients
  prior(cauchy(0, 1), class = "sd"),         # Standard deviations
  prior(lkj(2), class = "cor")               # Correlations
)


truncated_model_with_priors <- brm(
  bf(log_WAP | trunc(lb = log1p(0), ub = log1p(4)) ~ to_pfeml + (1 + to_pfeml | cntryyr)),
  data = merged_data,
  family = gaussian(),
  chains = 4,
  iter = 4000,
  warmup = 1000,
  cores = 4,
  save_model = "full_model_transformed.rds",
  prior = priors
)

#saveRDS(truncated_model_with_priors, file = "truncated_model_with_priors.rds")

# save to .RData
save(truncated_model_with_priors, file = "truncated_model_full.RData")

# quick summary
summary(truncated_model_with_priors)

# Plot autocorrelation
mcmc_plot <- mcmc_plot(truncated_model_with_priors, type = "acf")
save(mcmc_plot, file = "mcmc_plot_full.png")

pairs(truncated_model_with_priors)

pp_check(truncated_model_with_priors)
save(pp_check, file = "pp_check_full.png")

# Plot trace plots to inspect chains visually
truncated_model_full <- plot(truncated_model_with_priors)
save(truncated_model_with_priors, file = "truncated_model_plot1.png")

residuals <- residuals(truncated_model_with_priors)
plot(residuals)

```

```{r, correct plot}
# Define the sequence of to_pfeml values
to_pfeml_seq <- seq(from = min(merged_data$to_pfeml, na.rm = TRUE),
                    to = max(merged_data$to_pfeml, na.rm = TRUE),
                    length.out = 100)

# Extract unique levels of the grouping variable
unique_cntryyr <- unique(merged_data$cntryyr)

# Create a new data frame with the sequence of to_pfeml values for each level of cntryyr
new_data <- expand.grid(to_pfeml = to_pfeml_seq, cntryyr = unique_cntryyr)

# Function to process data in batches
batch_predict <- function(model, data, batch_size = 1000) {
  n <- nrow(data)
  results <- list()
  
  for (i in seq(1, n, by = batch_size)) {
    print(paste("Processing batch:", i, "to", min(i + batch_size - 1, n)))
    batch_data <- data[i:min(i + batch_size - 1, n), ]
    predictions <- posterior_predict(model, newdata = batch_data, draws = 100)
    results[[length(results) + 1]] <- predictions
  }
  
  do.call(cbind, results)
}

# Generate predictions for the new data in batches
predicted_samples <- batch_predict(truncated_model_with_priors, new_data, batch_size = 1000)

# Save memory by clearing the original data frame
#rm(merged_data)
#gc()

# Compute mean and confidence intervals for predictions across all cntryyr levels
predicted_mean <- apply(predicted_samples, 2, mean, na.rm = TRUE)
lower_ci <- apply(predicted_samples, 2, quantile, probs = 0.025, na.rm = TRUE)
upper_ci <- apply(predicted_samples, 2, quantile, probs = 0.975, na.rm = TRUE)

# Reshape the predicted mean and confidence intervals to match the sequence length
predicted_mean <- matrix(predicted_mean, ncol = length(to_pfeml_seq), byrow = TRUE)
lower_ci <- matrix(lower_ci, ncol = length(to_pfeml_seq), byrow = TRUE)
upper_ci <- matrix(upper_ci, ncol = length(to_pfeml_seq), byrow = TRUE)

# Average across all cntryyr levels
predicted_mean <- apply(predicted_mean, 2, mean, na.rm = TRUE)
lower_ci <- apply(lower_ci, 2, mean, na.rm = TRUE)
upper_ci <- apply(upper_ci, 2, mean, na.rm = TRUE)

# Plot mean predicted values with confidence intervals
predicted_WAP_batch <- plot(to_pfeml_seq, predicted_mean, type = "l",
     #ylim = range(c(lower_ci, upper_ci)),
     ylim = c(-0.5, 1.5),
     main = "Predicted WAP against Female Delegates Proportion",
     xlab = "Female Delegates Proportion",
     ylab = "Predicted WAP",
     col = "black", lwd = 2)
lines(to_pfeml_seq, lower_ci, col = "black", lty = 2)
lines(to_pfeml_seq, upper_ci, col = "black", lty = 2)

save(predicted_WAP_batch, file = "Predicted WAP batch.jpg")


# to save this use ggplot-plot
# Load ggplot2 package

# Create the plot using ggplot
predicted_WAP_plot <- ggplot(data = NULL, aes(x = to_pfeml_seq)) +
  geom_line(aes(y = predicted_mean), color = "black", size = 2) +
  geom_line(aes(y = lower_ci), linetype = "dashed", color = "black") +
  geom_line(aes(y = upper_ci), linetype = "dashed", color = "black") +
  ylim(-0.5, 1.5) +
  labs(title = "Predicted WAP against Female Delegates Proportion",
       x = "Female Delegates Proportion",
       y = "Predicted WAP")+
  theme_classic()

# Save the plot as a .jpg file
ggsave("Predicted_WAP_batch.jpg", plot = predicted_WAP_plot, width = 8, height = 6, dpi = 300)

```
