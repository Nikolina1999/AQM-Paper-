---
title: "Rausschmiss"
author: "Nikolina Filiposki"
date: "2024-05-11"
output: html_document
---


```{r, Holzweg}

dta$to_pfeml <- as.numeric(dta$to_pfeml)
#dta$country <- as.numeric(dta$country)
dta$year <- as.numeric(dta$year)
dta$prior_opposition <- as.numeric(dta$prior_opposition)

hist(dta$party_like)
ggplot(dta, aes(x = to_pfeml, y = party_like)) +
  geom_point() +  # Add points
  labs(x = "Women", y = "Party Like Score") +  # Add axis labels
  ggtitle("Scatterplot of Party Like Scores") +  # Add title
  theme_bw()

# Dummy for country 
dta$oceania <- ifelse(dta$country == "Australia"| dta$country == "New Zealand",1,0)

dta$autralia <- ifelse(dta$country == "Australia",1,0)
dta$nz <- ifelse(dta$country == "New Zealand",1,0)
dta$israel <- ifelse(dta$country == "Isreal",1,0)

dta$france <- ifelse(dta$country == "France",1,0)
dta$germany <- ifelse(dta$country == "Germany",1,0)
dta$austria <- ifelse(dta$country == "Austria",1,0)
dta$spain <- ifelse(dta$country == "Spain",1,0)
dta$portugal <- ifelse(dta$country == "Portugal",1,0)
dta$greece <- ifelse(dta$country == "Grecce",1,0)
dta$netherlands <- ifelse(dta$country == "Netherlands",1,0)
dta$switzerland <- ifelse(dta$country == "Switzerland",1,0)

dta$gb <- ifelse(dta$country == "Great Britain",1,0)
dta$ireland <- ifelse(dta$country == "Ireland",1,0)
dta$us <- ifelse(dta$country == "United States of America",1,0)
dta$canada <- ifelse(dta$country == "Canada",1,0)



dta$sweden <- ifelse(dta$country == "Sweden",1,0)
dta$norway <- ifelse(dta$country == "Norway",1,0)
dta$finland <- ifelse(dta$country == "Finland",1,0)
dta$iceland <- ifelse(dta$country == "Iceland",1,0)
dta$denmark <- ifelse(dta$country == "Denmark",1,0)

#dta$year

X <- cbind(dta$to_pfeml, dta$germany, dta$prior_opposition)

num_predictors <- ncol(X)
startvals <- c(rep(0, num_predictors), 0)

#startvals <-
#  c(rep(0, 3), 0)

# res <- optim(
#   startvals,
#   ll_ologit,
#   Z = Z,
#   X = X,
#   method = "L-BFGS-B",
#   control = list(fnscale = -1, trace = T, maxit = 10000),
#   hessian = TRUE
# )

#results <- cbind(res$par, sqrt(diag(solve(-res$hessian))))

# rownames(results) <- c("Women",
#                        "Country",
#                        "Year",
#                        "Prior opposition",
#                        "tau1",
#                        "tau2",
#                        "tau3")
# 
# colnames(results) <- c("Coef", "SE")
# 
# results

```

```{r}
#load("dyadic_data_1-4-22.Rdata")

#cses1 <- read_dta("raw-data/cses1.dta")
#rm(cses1)

# like <- cses1 %>%  select(A3020_A)
# like<- na.omit(like)
# rm(like)

 


multilevel_data$thermometer_score <- as.factor(multilevel_data$thermometer_score)

n <- polr(thermometer_score ~ to_pfeml, 
          data = multilevel_data, 
          Hess = TRUE,
          method = "logistic")

## view a summary of the model
summary(n)


o <- polr(thermometer_score ~ to_pfeml + cntryyr, 
          data = multilevel_data, 
          Hess = TRUE,
          method = "logistic")

## view a summary of the model
summary(o)

p <- glm(to_pfeml ~ thermometer_score, data = multilevel_data)
summary(p)


table(multilevel_data$match)
```

```{r, m party_like ~ to_pfeml, eval = FALSE}
dta$party_like <- as.factor(dta$party_like)

m <- polr(party_like ~ to_pfeml, 
          data = dta, 
          Hess = TRUE,
          method = "logistic")

## view a summary of the model
summary(m)
```

```{r}
ll_ologit <- function(theta, Z, X) {
  k <- ncol(X)
  J <- ncol(Z)
  
  # Subset theta
  beta <- theta[1:k]
  
  tau <- theta[(k + 1):(k + J - 1)]
  
  # Linear Predictor
  U <- X %*% beta
  
  # Probabilities in a matrix
  probs <- matrix(nrow = length(U), ncol = J)
  
  # Calculate Probabilities for the different tau values
  
  # The first category is different
  probs[, 1] <- 1 / (1 + exp(-(tau[1] - U)))
  
  # Now the categories in between
  for (j in 2:(J - 1)) {
    probs[, j] <- 1 / (1 + exp(-(tau[j] - U))) -
      1 / (1 + exp(-(tau[j - 1] - U)))
  }
  
  # And the last category is different again.
  probs[, J] <- 1 - 1 / (1 + exp(-(tau[J - 1] - U)))
  
  ll <- sum(log(probs[Z]))
  
  # sum over probabilities
  return(ll)
}

```

```{r, cats with dta$party_like}
# cats <- sort(unique(dta$party_like))  # Different categories
# J <- length(unique(dta$party_like))  # Number of categories
# 
# Z <-
#   matrix(NA, nrow = length(dta$party_like), ncol = J)  # Empty indicator matrix
# 
# for (j in 1:J) {
#   Z[, j] <- dta$party_like == cats[j]
# }
# 
# head(Z)

```


```{r, cats with multilevel_data$thermometer_score}
cats <- sort(unique(multilevel_data$thermometer_score))  # Different categories
J <- length(unique(multilevel_data$thermometer_score))  # Number of categories

Z <-
  matrix(NA, nrow = length(multilevel_data$thermometer_score), ncol = J)  # Empty indicator matrix

for (j in 1:J) {
  Z[, j] <- multilevel_data$thermometer_score == cats[j]
}

head(Z)
```


```{r}
cy_interest <- c("AUS_1996",
                 "AUS_2004",
                 "AUS_2007",
                 "AUS_2013",
                 "AUT_2008", 
                 "AUT_2013",
                 "AUT_2017",
                 "CAN_1997",
                 "CAN_2004",
                 "CAN_2008",
                 "CAN_2011",
                 "CAN_2015",
                 "DNK_1998",
                 "DNK_2001",
                 "DNK_2007",
                 "FIN_2003",
                 "FIN_2007",
                 "FIN_2011",
                 "FIN_2015", 
                 "FRA_2002", 
                 "FRA_2007",
                 "FRA_2012", 
                 "FRA_2017", 
                 "DEU_1998", 
                 "DEU12002", 
                 "DEU22002", 
                 "DEU_2005",
                 "DEU_2009", 
                 "DEU_2013",
                 "DEU_2017", 
                 "GBR_1997", 
                 "GBR_2005", 
                 "GBR_2015", 
                 "GBR_2017",
                 "GRC_2009", 
                 "GRC_2012", 
                 "GRC12015",
                 "GRC22015",
                 "ISL_1999", 
                 "ISL_2003",
                 "ISL_2007",
                 "ISL_2009",
                 "ISL_2013",
                 "ISL_2016",
                 "ISL_2017",
                 "IRL_2002",
                 "IRL_2007",
                 "IRL_2011",
                 "IRL_2016",
                 "ISR_1996",
                 "ISR_2003",
                 "ISR_2006",
                 "ISR_2013",
                 "NLD_1998",
                 "NLD_2002",
                 "NLD_2006",
                 "NLD_2010",
                 "NLD_2017",
                 "NZL_1996",
                 "NZL_2002",
                 "NZL_2008",
                 "NZL_2011",
                 "NZL_2014", 
                 "NZL_2017",
                 "NOR_1997",
                 "NOR_2001",
                 "NOR_2005",
                 "NOR_2009",
                 "NOR_2013",
                 "NOR_2017",
                 "PRT_2002",
                 "PRT_2005",
                 "PRT_2009",
                 "PRT_2015",
                 "ESP_1996",
                 "ESP_2000",
                 "ESP_2004",
                 "ESP_2008",
                 "SWE_1998",
                 "SWE_2002",
                 "SWE_2006",
                 "SWE_2014",
                 "CHE_1999",
                 "CHE_2003",
                 "CHE_2007",
                 "CHE_2011",
                 "USA_1996",
                 "USA_2004",
                 "USA_2008",
                 "USA_2012",
                 "USA_2016"
                 )
# IMD1008_MOD_1  >>> ID COMPONENT - CSES MODULE 1
# IMD1008_MOD_2  >>> ID COMPONENT - CSES MODULE 2
# IMD1008_MOD_3  >>> ID COMPONENT - CSES MODULE 3
# IMD1008_MOD_4  >>> ID COMPONENT - CSES MODULE 4
# IMD1008_MOD_5  >>> ID COMPONENT - CSES MODULE 5

# IMD1006_NAM

# IMD1008_YEAR


# AUS_1996. AUSTRALIA (1996)
# AUS_2004. AUSTRALIA (2004)
# AUS_2007. AUSTRALIA (2007)
# AUS_2013. AUSTRALIA (2013)

# AUT_2008. AUSTRIA (2008)
# AUT_2013. AUSTRIA (2013)
# AUT_2017. AUSTRIA (2017)

# CAN_1997. CANADA (1997)
# CAN_2004. CANADA (2004)
# CAN_2008. CANADA (2008)
# CAN_2011. CANADA (2011)
# CAN_2015. CANADA (2015)

# DNK_1998. DENMARK (1998)
# DNK_2001. DENMARK (2001)
# DNK_2007. DENMARK (2007)
  
# FIN_2003. FINLAND (2003)
# FIN_2007. FINLAND (2007)
# FIN_2011. FINLAND (2011)
# FIN_2015. FINLAND (2015)

# FRA_2002. FRANCE (2002)
# FRA_2007. FRANCE (2007)
# FRA_2012. FRANCE (2012)
# FRA_2017. FRANCE (2017)

# DEU_1998. GERMANY (1998)
# DEU12002. GERMANY (2002 Telephone)
# DEU22002. GERMANY (2002 Mail-Back)
# DEU_2005. GERMANY (2005)
# DEU_2009. GERMANY (2009)
# DEU_2013. GERMANY (2013)
# DEU_2017. GERMANY (2017)
             
# GBR_1997. GREAT BRITAIN (1997)
# GBR_2005. GREAT BRITAIN (2005)
# GBR_2015. GREAT BRITAIN (2015)
# GBR_2017. GREAT BRITAIN (2017)

# GRC_2009. GREECE (2009)
# GRC_2012. GREECE (2012)
# GRC12015. GREECE (2015 Jan)
# GRC22015. GREECE (2015 Sep)

# ISL_1999. ICELAND (1999)
# ISL_2003. ICELAND (2003)
# ISL_2007. ICELAND (2007)
# ISL_2009. ICELAND (2009)
# ISL_2013. ICELAND (2013)
# ISL_2016. ICELAND (2016)
# ISL_2017. ICELAND (2017)

# IRL_2002. IRELAND (2002)
# IRL_2007. IRELAND (2007)
# IRL_2011. IRELAND (2011)
# IRL_2016. IRELAND (2016)
             
# ISR_1996. ISRAEL (1996)
# ISR_2003. ISRAEL (2003)
# ISR_2006. ISRAEL (2006)
# ISR_2013. ISRAEL (2013)
             
# NLD_1998. NETHERLANDS (1998)
# NLD_2002. NETHERLANDS (2002)
# NLD_2006. NETHERLANDS (2006)
# NLD_2010. NETHERLANDS (2010)
# NLD_2017. NETHERLANDS (2017)
             
# NZL_1996. NEW ZEALAND (1996)
# NZL_2002. NEW ZEALAND (2002)
# NZL_2008. NEW ZEALAND (2008)
# NZL_2011. NEW ZEALAND (2011)
# NZL_2014. NEW ZEALAND (2014)
# NZL_2017. NEW ZEALAND (2017)
             
# NOR_1997. NORWAY (1997)
# NOR_2001. NORWAY (2001)
# NOR_2005. NORWAY (2005)
# NOR_2009. NORWAY (2009)
# NOR_2013. NORWAY (2013)
# NOR_2017. NORWAY (2017)
             
# PRT_2002. PORTUGAL (2002)
# PRT_2005. PORTUGAL (2005)
# PRT_2009. PORTUGAL (2009)
# PRT_2015. PORTUGAL (2015)
             
# ESP_1996. SPAIN (1996)
# ESP_2000. SPAIN (2000)
# ESP_2004. SPAIN (2004)
# ESP_2008. SPAIN (2008)
             
# SWE_1998. SWEDEN (1998)
# SWE_2002. SWEDEN (2002)
# SWE_2006. SWEDEN (2006)
# SWE_2014. SWEDEN (2014)
             
# CHE_1999. SWITZERLAND (1999)
# CHE_2003. SWITZERLAND (2003)
# CHE_2007. SWITZERLAND (2007)
# CHE_2011. SWITZERLAND (2011)
             
# USA_1996. UNITED STATES (1996)
# USA_2004. UNITED STATES (2004)
# USA_2008. UNITED STATES (2008)
# USA_2012. UNITED STATES (2012)
# USA_2016. UNITED STATES (2016)



# IMD3008_A     >>> LIKE-DISLIKE - PARTY A
# IMD3008_B     >>> LIKE-DISLIKE - PARTY B
# IMD3008_C     >>> LIKE-DISLIKE - PARTY C
# IMD3008_D     >>> LIKE-DISLIKE - PARTY D
# IMD3008_E     >>> LIKE-DISLIKE - PARTY E
# IMD3008_F     >>> LIKE-DISLIKE - PARTY F
# IMD3008_G     >>> LIKE-DISLIKE - PARTY G (OPTIONAL)
# IMD3008_H     >>> LIKE-DISLIKE - PARTY H (OPTIONAL)
# IMD3008_I     >>> LIKE-DISLIKE - PARTY I (OPTIONAL)

# IMD5000_A         >>>    PARTY A IDENTIFIER - NUMERICAL
# IMD5000_B         >>>    PARTY B IDENTIFIER - NUMERICAL
# IMD5000_C         >>>    PARTY C IDENTIFIER - NUMERICAL
# IMD5000_D         >>>    PARTY D IDENTIFIER - NUMERICAL
# IMD5000_E         >>>    PARTY E IDENTIFIER - NUMERICAL
# IMD5000_F         >>>    PARTY F IDENTIFIER - NUMERICAL
# IMD5000_G         >>>    PARTY G IDENTIFIER - NUMERICAL
# IMD5000_H         >>>    PARTY H IDENTIFIER - NUMERICAL
# IMD5000_I         >>>    PARTY I IDENTIFIER - NUMERICAL

# IMD5001_A         >>>    PERCENT VOTE - LOWER HOUSE - PARTY A
# IMD5001_B         >>>    PERCENT VOTE - LOWER HOUSE - PARTY B
# IMD5001_C         >>>    PERCENT VOTE - LOWER HOUSE - PARTY C
# IMD5001_D         >>>    PERCENT VOTE - LOWER HOUSE - PARTY D
# IMD5001_E         >>>    PERCENT VOTE - LOWER HOUSE - PARTY E
# IMD5001_F         >>>    PERCENT VOTE - LOWER HOUSE - PARTY F
# IMD5001_G         >>>    PERCENT VOTE - LOWER HOUSE - PARTY G
# IMD5001_H         >>>    PERCENT VOTE - LOWER HOUSE - PARTY H
# IMD5001_I         >>>    PERCENT VOTE - LOWER HOUSE - PARTY I

# IMD5003_A         >>>    PERCENT VOTE - UPPER HOUSE - PARTY A
# IMD5003_B         >>>    PERCENT VOTE - UPPER HOUSE - PARTY B
# IMD5003_C         >>>    PERCENT VOTE - UPPER HOUSE - PARTY C
# IMD5003_D         >>>    PERCENT VOTE - UPPER HOUSE - PARTY D
# IMD5003_E         >>>    PERCENT VOTE - UPPER HOUSE - PARTY E
# IMD5003_F         >>>    PERCENT VOTE - UPPER HOUSE - PARTY F
# IMD5003_G         >>>    PERCENT VOTE - UPPER HOUSE - PARTY G
# IMD5003_H         >>>    PERCENT VOTE - UPPER HOUSE - PARTY H
# IMD5003_I         >>>    PERCENT VOTE - UPPER HOUSE - PARTY I

# IMD5005_A         >>>    PERCENT VOTE - PRESIDENT - PARTY A
# IMD5005_B         >>>    PERCENT VOTE - PRESIDENT - PARTY B
# IMD5005_C         >>>    PERCENT VOTE - PRESIDENT - PARTY C
# IMD5005_D         >>>    PERCENT VOTE - PRESIDENT - PARTY D
# IMD5005_E         >>>    PERCENT VOTE - PRESIDENT - PARTY E
# IMD5005_F         >>>    PERCENT VOTE - PRESIDENT - PARTY F
# IMD5005_G         >>>    PERCENT VOTE - PRESIDENT - PARTY G
# IMD5005_H         >>>    PERCENT VOTE - PRESIDENT - PARTY H
# IMD5005_I         >>>    PERCENT VOTE - PRESIDENT - PARTY I
```



```{r}
# Merge datasets on common identifier
merged_dataset <- merge(multilevel_data, selected_d, by.x = "ID", by.y = "ID", all = TRUE) 

print(multilevel_data$ID)
class(multilevel_data$ID)
print(selected_d$ID)
class(selected_d$ID)

sum(is.na(multilevel_data$ID))
sum(is.na(selected_d$ID))

	
merged_dataset <- left_join(selected_d, multilevel_data, by = "ID")
```


```{r}
# Clean the likability variables


# Filter out rows with values 96, 97, and 98 from variables IMD3008_A to IMD3008_I
# Also filter out rows with value 99 in variables IMD3008_A to IMD3008_F
selected_d <- selected_dataset %>%
  filter(across(starts_with("IMD3008_"), 
                ~ if (endsWith(cur_column(), "_A") | 
                      endsWith(cur_column(), "_B") |
                      endsWith(cur_column(), "_C") |
                      endsWith(cur_column(), "_D") |
                      endsWith(cur_column(), "_E") |
                      endsWith(cur_column(), "_F")) {
                  !. %in% c(96, 97, 98, 99)
                } else {
                  !. %in% c(96, 97, 98)
                }))
```

```{r}
# Clean the likability variables


# Filter out rows with values 96, 97, and 98 from variables IMD3008_A to IMD3008_I
# Also filter out rows with value 99 in variables IMD3008_A to IMD3008_F
merged_dataset <- merged_dataset %>%
  filter(across(starts_with("IMD3008_"), 
                ~ if (endsWith(cur_column(), "_A") | 
                      endsWith(cur_column(), "_B") |
                      endsWith(cur_column(), "_C") |
                      endsWith(cur_column(), "_D") |
                      endsWith(cur_column(), "_E") |
                      endsWith(cur_column(), "_F")) {
                  !. %in% c(96, 97, 98, 99)
                } else {
                  !. %in% c(96, 97, 98)
                }))

vars_merge <- select(merged_dataset,
                     "IMD1004",
                     "party_like",
                     "party_dislike",
                     "cntryyr",
                     "to_pfeml",
                     starts_with("IMD5000_"),
                     starts_with("IMD5001_"), 
                     starts_with("IMD5003_"), 
                     starts_with("IMD5005_"),
                     starts_with("IMD3008_"),
                     )



``$$

\text{Distance}_{ijk} = \sqrt{\sum_{p=1}^{P} v_p \times (\text{like}_{ikp} - \text{like}_{jkp})^2}

$$

```{r, Distance jk}
calculate_dyadic_mean_distance <- function(data) {
  # List of party preference variables (IMD3008_A to IMD3008_I)
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  # List of vote share variables (IMD5001_A to IMD5001_I)
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  # Initialize a matrix to store dyadic mean-distance measures
  dyadic_mean_distance <- matrix(NA, nrow = nrow(data), ncol = length(party_pref_vars)^2)
  
  # Loop through each respondent
  for (i in 1:nrow(data)) {
    # Get party preference and vote share data for the current respondent
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Exclude missing values (99)
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    
    # Loop through each pair of parties
    for (j in 1:length(party_pref_vars)) {
      for (k in 1:length(party_pref_vars)) {
        # Calculate the difference in like-dislike scores between the pair of parties
        difference <- party_pref[j] - party_pref[k]
        
        # Weight the difference by the vote share of the corresponding party
        weighted_difference <- difference * vote_share[j]
        
        # Square the weighted difference
        squared_weighted_difference <- weighted_difference^2
        
        # Store the squared weighted difference in the matrix
        dyadic_mean_distance[i, (j - 1) * length(party_pref_vars) + k] <- squared_weighted_difference
      }
    }
  }
  
  # Take the square root of the sum of squared weighted differences to obtain the dyadic mean-distance measure
  dyadic_mean_distance <- sqrt(rowSums(dyadic_mean_distance, na.rm = TRUE))
  
  # Return the dyadic mean-distance measures for each respondent
  return(dyadic_mean_distance)
}

dyadic_distance <- calculate_dyadic_mean_distance(selected_d)
print(dyadic_distance)
````



# Load the data 
```{r, data}
load("raw-data/dyadic_data_1-4-22.Rdata")
dta <- updated_data 
```

# Get rid of unneeded variables 
```{r, variables}
vars <- c("to_mp_number", "to_rile", "to_economy", "to_society", "year", "country", 
          "to_pfeml", "to_femaleleader")
dta <- dta[vars]
dta <- na.omit(dta)
```

# Identiy unique parties being evaluated
```{r, party variales}
dta_unique <- unique(dta)
```


# First Figure 
```{r, figure no. 1}
fig1 <- ggplot(dta_unique, aes(x = to_pfeml)) +
  geom_histogram(color="black", fill="grey40", binwidth =0.1, center=0.25) +
  scale_x_continuous(breaks = seq(0,1,0.1)) +
  theme_minimal() +
  theme(plot.title = element_text(size=12)) +
  ylab("Frequency")+
  xlab("Proportion of Women MPs");fig1

```

# CREATING TABLE 1 COLUMNS 1 & 2 

```{r, Table 1 and Table 2, result = 'axis'}
#Out party % women, non-clustered SEs
load("raw-data/dyadic_data_1-4-22.Rdata")

dta <-updated_data

#creating the country-year fixed effects
#dta$cntryyr <-paste(dta$country, dta$year, sep = "")

## Removing smaller parties
dta <- subset(dta, dta$to_prior_seats >=4)


vars <- c("rile_distance_s", "prior_coalition", "prior_opposition", "econ_distance_s", "society_distance_s",
          "year", "country", "party_dislike", "party_like", "to_pfeml", "to_prior_seats", "to_mp_number")
# "cntryyr"
dta <- dta[vars]
dta <- na.omit(dta)


#table1.1 <-lm(party_like ~ to_pfeml + as.factor(cntryyr), data = dta)
#table1.2 <-lm(party_like ~ to_pfeml + rile_distance_s + prior_coalition + prior_opposition + as.factor(cntryyr), data = dta)

### With clustered SEs
 #stargazer(table1.1, table1.2, 
 #          header = F, 
 #          add.lines = list(c("Country-Year Fixed Effects?", "Yes"), c("Country-Level  #Clustered SEs?", "Yes")),
 #          se = starprep(table1.1, table1.2,
 #                        clusters = dta$country),
 #          keep = c("to_pfeml", "rile_distance_s", "prior_coalition",     #"prior_opposition", 
 #                   "econ_distance_s", "society_distance_s"))
rm(dta_unique)
```


```{r}
#Out party % women, non-clustered SEs
load("raw-data/dyadic_data_1-4-22.Rdata")

load("raw-data/multilevel_1-5-22.Rdata")
```

```{r}
head(dta)

par(mfrow = c(2,2))

hist(dta$party_like,
        border = F,
        main = "Party Like",
        cex.main = 0.8,
        las = 1)

hist(dta$party_dislike,
        main = "Party Dislike",
        border = F,
        cex.main = 0.8,
        las = 1)

hist(dta$to_pfeml,
        main = "Percentage of women",
        border = F,
        cex.main = 0.8,
        las = 1)

hist(table(multilevel_data$thermometer_score),
        main = "Thermometer Score",
        border = F,
        cex.main = 0.8,
        las = 1)


```



```{r, normalization (wrong)}
# Function for normalizing dyadic affective polarization scores
normalize_dyadic_scores <- function(scores) {
  max_scores <- apply(scores, 1, max)  # Compute the maximum score for each respondent
  normalized_scores <- scores / max_scores  # Scale scores relative to each respondent's maximum score
  return(normalized_scores)
}

# Example usage:
# Assuming weighted_AP_dyadic contains the affective polarization scores
normalized_dyadic_scores <- normalize_dyadic_scores(weighted_AP_dyadic)
print(normalized_dyadic_scores)



```


```{r, WAP (non-dyadic) old one }
calculate_weighted_AP <- function(data) {
  # List of party preference variables (IMD3008_A to IMD3008_I)
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  # List of vote share variables (IMD5001_A to IMD5001_I)
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  # Initialize a vector to store weighted AP scores for each respondent
  weighted_AP_scores <- numeric(nrow(data))
  
  # Loop through each respondent
  for (i in 1:nrow(data)) {
    # Get party preference and vote share data for the current respondent
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Initialize variables for calculating weighted average party affect and spread
    likei <- 0
    spread_i <- 0
    
    # Loop through each party preference
    for (j in 1:length(party_pref)) {
      # Skip value 99
      if (party_pref[j] == 99) next
      
      # Calculate the weighted average party affect for the current respondent
      likei <- likei + party_pref[j] * vote_share[j]
    }
    
    # Calculate the spread for the current respondent
    for (j in 1:length(party_pref)) {
      # Skip value 99
      if (party_pref[j] == 99) next
      
      spread_i <- spread_i + (party_pref[j] - likei)^2 * vote_share[j]
    }
    spread_i <- sqrt(spread_i)
    
    # Store the weighted AP score for the current respondent
    weighted_AP_scores[i] <- spread_i
  }
  
  # Return the weighted AP scores for each respondent
  return(weighted_AP_scores)
}

weighted_AP_scores <- calculate_weighted_AP(selected_d)
print(weighted_AP_scores)



```

$$
\text{WAP}_{ip} = \sqrt{v_p \times (\text{like}_{ip} - \bar{\text{like}}_i)^2}

$$



```{r, WAP dyadic old one }
calculate_weighted_AP_dyadic <- function(data) {
  # List of party preference variables (IMD3008_A to IMD3008_I)
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  # List of vote share variables (IMD5001_A to IMD5001_I)
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  # Initialize a matrix to store weighted AP scores for each respondent-party pair
  num_respondents <- nrow(data)
  num_parties <- length(party_pref_vars)
  weighted_AP_dyadic <- matrix(0, nrow = num_respondents, ncol = num_parties)
  
  # Loop through each respondent
  for (i in 1:num_respondents) {
    # Get party preference and vote share data for the current respondent
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Loop through each party preference
    for (j in 1:num_parties) {
      # Skip value 99
      if (party_pref[j] == 99) next
      
      # Calculate the weighted average party affect for the current respondent
      likei <- sum(party_pref * vote_share)
      
      # Calculate the spread for the current respondent-party pair
      spread_ij <- sqrt((party_pref[j] - likei)^2 * vote_share[j])
      
      # Store the weighted AP score for the current respondent-party pair
      weighted_AP_dyadic[i, j] <- spread_ij
    }
  }
  
  # Return the matrix of weighted AP scores for each respondent-party pair
  return(weighted_AP_dyadic)
}

weighted_AP_dyadic <- calculate_weighted_AP_dyadic(selected_d)
print(weighted_AP_dyadic)

```


```{r try to add column 10 and divide data by it }

add_non_zero_counts <- function(weighted_AP_dyadic) {
  # Count non-zero party preferences for each respondent
  non_zero_counts <- apply(weighted_AP_dyadic != 0, 1, sum)
  
  # Add non-zero counts as column 10
  weighted_AP_dyadic <- cbind(weighted_AP_dyadic, non_zero_counts)
  
  return(weighted_AP_dyadic)
}

weighted_AP_dyadic <- add_non_zero_counts(weighted_AP_dyadic)
print(weighted_AP_dyadic)

weighted_AP_dyadic <- cbind(weighted_AP_dyadic, 
                            matrix(NA, 
                                   nrow = nrow(weighted_AP_dyadic),
                                   ncol = length(11:19)))

calculate_division <- function(weighted_AP_dyadic) {
  # Get the non-zero counts from column 10
  non_zero_counts <- weighted_AP_dyadic[, 10]
  
  # Divide values for each respondent's party preferences row-wise and store in new columns 11-19
  for (i in 1:nrow(weighted_AP_dyadic)) {
    # Get the values for the respondent
    respondent_values <- weighted_AP_dyadic[i, 1:9]
    
    # Calculate the division factors (non-zero counts for the respective respondent)
    division_factors <- non_zero_counts[i]
    
    # Initialize new column indices
    new_column_indices <- 11:19
    
    # Divide values by the division factor and store in new columns
    # Skip division if the party preference is zero
    for (j in 1:9) {
      if (respondent_values[j] != 0) {
        weighted_AP_dyadic[i, new_column_indices[j]] <- respondent_values[j] / division_factors
      } else {
        weighted_AP_dyadic[i, new_column_indices[j]] <- 0
      }
    }
  }
  
  return(weighted_AP_dyadic)
}




weighted_AP_dyadic <- calculate_division(weighted_AP_dyadic)
print(weighted_AP_dyadic)

```



 Try Ollis approach: Taking the 1/n within likei 
 
```{r olis fisrt approch idea with 1/n rp rated }
calculate_weighted_AP_dyadic <- function(data) {
  # List of party preference variables (IMD3008_A to IMD3008_I)
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  # List of vote share variables (IMD5001_A to IMD5001_I)
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  # Initialize a matrix to store weighted AP scores for each respondent-party pair
  num_respondents <- nrow(data)
  weighted_AP_dyadic <- matrix(0, nrow = num_respondents, ncol = length(party_pref_vars))
  
  # Loop through each respondent
  for (i in 1:num_respondents) {
    # Get party preference and vote share data for the current respondent
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Get the number of rated parties for the current respondent
    num_rated_parties <- sum(party_pref != 99)
    
    # Loop through each party preference
    for (j in 1:length(party_pref_vars)) {
      # Skip value 99
      if (party_pref[j] == 99) next
      
      # Calculate the weighted average party affect for the current respondent
      likei <- sum(party_pref * vote_share) / num_rated_parties
      
      # Calculate the spread for the current respondent-party pair
      spread_ij <- sqrt((party_pref[j] - likei)^2 * vote_share[j])
      
      # Store the weighted AP score for the current respondent-party pair
      weighted_AP_dyadic[i, j] <- spread_ij
    }
  }
  
  # Return the matrix of weighted AP scores for each respondent-party pair
  return(weighted_AP_dyadic)
}

weighted_AP_dyadic <- calculate_weighted_AP_dyadic(selected_d)
print(weighted_AP_dyadic)


```
 
 
 Ollis second idea 

$$
\text{like}_{ip} - \bar{\text{like}}_i
$$

$$

like_i = \sum_{p=1}^{P} (v_p \times like_{ip})
$$


```{r, olli approach number two without sqrt and square}
calculate_weighted_AP_dyadic <- function(data) {
  # List of party preference variables (IMD3008_A to IMD3008_I)
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  # List of vote share variables (IMD5001_A to IMD5001_I)
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  # Initialize a matrix to store weighted AP scores for each respondent-party pair
  num_respondents <- nrow(data)
  weighted_AP_dyadic <- matrix(0, nrow = num_respondents, ncol = length(party_pref_vars))
  
  # Loop through each respondent
  for (i in 1:num_respondents) {
    # Get party preference and vote share data for the current respondent
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Get the number of rated parties for the current respondent
    num_rated_parties <- sum(party_pref != 99)
    
    # Loop through each party preference
    for (j in 1:length(party_pref_vars)) {
      # Skip value 99
      if (party_pref[j] == 99) next
      
      # Calculate the weighted average party affect for the current respondent
      likei <- sum(party_pref * vote_share)/ num_rated_parties
      
      # Calculate the spread for the current respondent-party pair
      spread_ij <- (party_pref[j] - likei)
      
      # Store the weighted AP score for the current respondent-party pair
      weighted_AP_dyadic[i, j] <- spread_ij
    }
  }
  
  # Return the matrix of weighted AP scores for each respondent-party pair
  return(weighted_AP_dyadic)
}

weighted_AP_dyadic <- calculate_weighted_AP_dyadic(selected_d)
print(weighted_AP_dyadic)

```




```{r oli approch with normalization}
calculate_weighted_AP_dyadic <- function(data) {
  # List of party preference variables (IMD3008_A to IMD3008_I)
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  # List of vote share variables (IMD5001_A to IMD5001_I)
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  # Initialize a matrix to store weighted AP scores for each respondent-party pair
  num_respondents <- nrow(data)
  num_parties <- length(party_pref_vars)
  weighted_AP_dyadic <- matrix(0, nrow = num_respondents, ncol = num_parties)
  
  # Loop through each respondent
  for (i in 1:num_respondents) {
    # Get party preference and vote share data for the current respondent
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Loop through each party preference
    for (j in 1:num_parties) {
      # Skip value 99
      if (party_pref[j] == 99) next
      
      # Calculate the weighted average party affect for the current respondent
      likei <- sum(party_pref * vote_share)
      
      # Calculate the spread for the current respondent-party pair
      spread_ij <- sqrt((party_pref[j] - likei)^2 * vote_share[j])
      
      # Store the weighted AP score for the current respondent-party pair
      weighted_AP_dyadic[i, j] <- spread_ij
    }
  }
  
  # Normalize the matrix by dividing by the maximum value of each row and scaling
  for (i in 1:num_respondents) {
    max_value_row <- max(weighted_AP_dyadic[i, ])
    if (max_value_row != 0) {
      weighted_AP_dyadic[i, ] <- weighted_AP_dyadic[i, ] / max_value_row * 10 # Scale to a range approximately from 0 to 10
    }
  }
  
  # Return the matrix of normalized weighted AP scores for each respondent-party pair
  return(weighted_AP_dyadic)
}

# Example usage
# Assuming `selected_d` is your data frame with appropriate columns
normalized_AP_dyadic <- calculate_weighted_AP_dyadic(selected_d)
print(normalized_AP_dyadic)


```




```{r, WAP dyadic (previous model), not realted to olis}
calculate_affective_polarization_party <- function(data, scaling_factor) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars))
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Store the scaled affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row * scaling_factor
  }
  
  return(affective_polarization_scores)
}

# Example usage:
scaling_factor <- 0.1  # Adjust the scaling factor as needed
affective_polarization_party_scores <- calculate_affective_polarization_party(selected_d, scaling_factor)
print(affective_polarization_party_scores)

```


```{r, account for value 9.99 in vote share (not correct) }
calculate_affective_polarization_party <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars_1 <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                         "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  vote_share_vars_2 <- c("IMD5003_A", "IMD5003_B", "IMD5003_C", "IMD5003_D", 
                         "IMD5003_E", "IMD5003_F", "IMD5003_G", "IMD5003_H", "IMD5003_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    
    # Check if vote share variables from IMD5001 are available, otherwise use IMD5003
    if (all(is.na(data[i, vote_share_vars_1]))) {
      vote_share <- unlist(data[i, vote_share_vars_2])
    } else {
      vote_share <- unlist(data[i, vote_share_vars_1])
    }
    
    # Exclude problematic vote shares
    problematic_vote_shares <- c(9.99, 9.97)
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Identify valid ratings excluding problematic vote shares
    valid_ratings <- which(party_pref != 997 & party_pref != 999 & !vote_share %in% problematic_vote_shares)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- which(party_pref != 99 & !vote_share %in% problematic_vote_shares)
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  # Optionally set column names
  #colnames(affective_polarization_scores) <- c("ID", paste0("Affective_Polarization_", LETTERS[1:length(party_pref_vars)]))
  
  return(affective_polarization_scores)
}


affective_polarization_party_scores <- calculate_affective_polarization_party(selected_d, selected_d$IMD1005)
print(affective_polarization_party_scores)
```


```{r, experimnet not good yet}
calculate_affective_polarization_party <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Check if all vote shares are 9.97, if so, use IMD5005_A to IMD5005_I
    if(all(vote_share == 9.97)) {
      vote_share <- unlist(data[i, paste0("IMD5005_", LETTERS[1:length(party_pref_vars)])])
    }
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- party_pref != 99
    for(j in 1:length(party_pref)) {
      if(valid_parties[j]) {
        if(vote_share[j] %in% c(9.990, 9.97, 9.99)) {
          affective_polarization_row[j] <- 0  # Skip problematic vote shares
        } else {
          affective_polarization_row[j] <- sqrt(vote_share[j] * (party_pref[j] - likei)^2)
        }
      } else {
        affective_polarization_row[j] <- 0  # Skip missing party preferences
      }
    }
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  return(affective_polarization_scores)
}


affective_polarization_party_scores <- calculate_affective_polarization_party(selected_d, selected_d$IMD1005)
print(affective_polarization_party_scores)

```

```{r, accounting for france, wrong}
calculate_affective_polarization_party <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Check if all vote shares are 9.97, if so, use IMD5005_A to IMD5005_I
    if(all(vote_share == 9.97)) {
      vote_share <- unlist(data[i, paste0("IMD5005_", LETTERS[1:length(party_pref_vars)])])
    }
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  return(affective_polarization_scores)
}

```

```{r experiment accounting fro wrong vote share (wrong)}
calculate_affective_polarization_party <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Check if all vote shares are 9.97, if so, use IMD5005_A to IMD5005_I
    if(all(vote_share == 9.97)) {
      vote_share <- unlist(data[i, paste0("IMD5005_", LETTERS[1:length(party_pref_vars)])])
    }
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    valid_vote_shares <- which(!is.na(vote_share) & !is.infinite(vote_share) & !(vote_share %in% c(9.990, 9.97, 9.99)))
    valid_combined <- intersect(valid_ratings, valid_vote_shares)
    likei <- sum(party_pref[valid_combined] * vote_share[valid_combined], na.rm = TRUE)
    
    # Calculate the affective polarization score for non-missing party preferences and valid vote shares
    for(j in 1:length(party_pref)) {
      if(party_pref[j] != 99 && !is.na(vote_share[j]) && !is.infinite(vote_share[j]) && !(vote_share[j] %in% c(9.990, 9.97, 9.99))) {
        affective_polarization_row[j] <- sqrt(vote_share[j] * (party_pref[j] - likei)^2)
      } else {
        affective_polarization_row[j] <- 0  # Skip invalid party preferences or vote shares
      }
    }
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  return(affective_polarization_scores)
}


```


```{r, AP for six cases where vote share and preferences differ}
calculate_AP_fail <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Filter out vote shares of 9.97, 9.99, and 9.990
    valid_vote_shares <- !vote_share %in% c(9.97, 9.99, 9.990)
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99 & valid_vote_shares)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- party_pref != 99 & valid_vote_shares
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  return(affective_polarization_scores)
}


AP_Fail_scores <- calculate_AP_fail(Fail_selected_d, Fail_selected_d$IMD1005)
print(AP_Fail_scores)
```


```{r}
calculate_AP_fail <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Find indices where both party preferences and vote shares are valid
    valid_indices <- which(!is.na(party_pref) & party_pref != 99 & !is.na(vote_share) 
                           & !vote_share %in% c(9.97, 9.99, 9.990))
    
    # Skip calculation if there are no valid indices
    if (length(valid_indices) == 0) {
      next
    }
    
    # Calculate the average like-dislike score for the current respondent
    likei <- sum(party_pref[valid_indices] * vote_share[valid_indices])
    
    # Calculate the affective polarization score for non-missing party preferences
    affective_polarization_row[valid_indices] <- sqrt(vote_share[valid_indices] * (party_pref[valid_indices] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  return(affective_polarization_scores)
}



AP_Fail_scores <- calculate_AP_fail(Fail_selected_d, Fail_selected_d$IMD1005)
print(AP_Fail_scores)

```


```{r, simulation function}

sim_function <- function(lm_obj, nsim = 1000, scenario){
  
  # Step 1: Get the regression coefficients
  beta_hat <- coef(lm_obj)
  
  # Step 2: Generate sampling distribution
  
  # Step 2.1: Get the variance-covariance matrix.
  V_hat <-  vcov(lm_obj) 
  
  # Step 2.2: Draw from the multivariate normal distribution.
  S <- mvrnorm(nsim, beta_hat, V_hat)

  # Step 3: Choose interesting covariate values. 
  # Make sure the matrix multiplication also works for single scenarios
  if(is.null(nrow(scenario))){
    scenario <- matrix(scenario, nrow = 1)
  }
  
  # Print a message if the scenario does not fit the regression.
  if(ncol(scenario) != length(lm_obj$coefficients)){
    return(cat("The scenario has the wrong number of variables."))
  } 
  
  # Step 4: Calculate Quantities of Interest - 
  # Expected Values
  EV <- S %*% t(scenario)
  return(EV)
}

```

```{r, scenario}

s1 <- c(1, 0.15, rep(0, 79))
s2 <- c(1, 0.5, rep(0, 79))

ev1 <- sim_function(table1, scenario = s1)
ev2 <- sim_function(table1, scenario = s2)

fd <- ev2- ev1

```




```{r}
# Fit the model
table1 <- lm(WAP ~ to_pfeml + as.factor(cntryyr), data = merged_data)
summary(table1)

# Simulation parameters
nsim <- 10000

options(digits = 10)

# Step 1: Extract the regression coefficients (fixed effects only)
beta_hat <- coef(table1)

# Step 2: Generate sampling distribution

# Step 2.1: Get the variance-covariance matrix
V_hat <- vcov(table1)

# Step 2.2: Draw from the multivariate normal distribution
S <-mvrnorm(nsim, beta_hat, V_hat)

# Step 3: Choose interesting covariate values for the scenario
class(merged_data$to_pfeml)

#min_to_pfeml <- min(merged_data$to_pfeml[merged_data$cntryyr == "Great Britain2015"])
#max_to_pfeml <- max(merged_data$to_pfeml[merged_data$cntryyr == "Great Britain2015"])
#to_pfeml_seq <- seq(min_to_pfeml, max_to_pfeml, length.out = 100)


# Using dplyr for filtering and calculating min and max
filtered_data <- merged_data %>%
  filter(cntryyr == "Great Britain2015") %>%
  summarise(min_to_pfeml = min(to_pfeml, na.rm = TRUE),
            max_to_pfeml = max(to_pfeml, na.rm = TRUE))

# Extract min and max values
min_to_pfeml <- filtered_data$min_to_pfeml
max_to_pfeml <- filtered_data$max_to_pfeml

to_pfeml_seq <- seq(min_to_pfeml, max_to_pfeml, length.out = 100)

# Extract the relevant cntryyr coefficient
cntryyr_coef <-  2.17599

# Include the intercept, to_pfeml, and the cntryyr coefficient for "Great Britain2015"
scenario <- cbind(1, to_pfeml_seq,  cntryyr_coef, length(to_pfeml_seq))

# Step 5: Calculate Expected Values (EV)
EV <- S %*% t(scenario)

# Step 6: Summarize the results
ev_mean <- apply(EV, 2, mean)
ev_ci <- apply(EV, 2, quantile, c(0.025, 0.975))

# You can now visualize the results
plot(to_pfeml_seq, ev_mean, type = "l", ylim = range(ev_ci), xlab = "to_pfeml", ylab = "Expected WAP")
lines(to_pfeml_seq, ev_ci[1,], col = "red", lty = 2)
lines(to_pfeml_seq, ev_ci[2,], col = "red", lty = 2)

```

First Reijan(2020)
```{r, Reijan 2020}
calculate_AP <- function(data) {
  # List of party preference variables (IMD3008_A to IMD3008_I)
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  # List of vote share variables (IMD5001_A to IMD5001_I)
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  # Initialize a vector to store AP scores for each party
  AP_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars),
                      dimnames = list(NULL, party_pref_vars))
  
  # Loop through each respondent
  for (i in 1:nrow(data)) {
    # Get party preference and vote share data for the current respondent
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Exclude missing values (99)
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    
    # Calculate the AP score for each party
    for (j in 1:length(party_pref_vars)) {
      if (j %in% valid_ratings) {
        in_party_pref <- party_pref[j]
        in_party_vote_share <- vote_share[j]
        combined_out_party_vote_share <- sum(vote_share[-j])
        out_party_prefs <- party_pref[-j]
        
        # Calculate the weighted difference between in-party and out-party preferences
        weighted_diff <- (in_party_pref - out_party_prefs) * (in_party_vote_share - combined_out_party_vote_share)
        
        # Sum up the weighted differences
        AP_scores[i, j] <- sum(weighted_diff)
      }
    }
  }
  
  # Return the relative AP scores for each party
  return(AP_scores)
}

# Selected_d is the dataset containing party preferences and vote shares
AP_scores <- calculate_AP(selected_d)
print(AP_scores)

#rm(AP_scores)
```



$$
\begin{equation}
Distance_i = \sqrt{\frac{1}{n_p} \sum_{p=1}^{P} v_p \times (like_{ip} - like_{\text{max},i})^2}
\end{equation}

$$


```{r, distance i}
calculate_mean_distance <- function(data) {
  # List of party preference variables (IMD3008_A to IMD3008_I)
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  # List of vote share variables (IMD5001_A to IMD5001_I)
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  # Initialize a vector to store mean-distance scores for each respondent
  mean_distance_scores <- numeric(nrow(data))
  
  # Loop through each respondent
  for (i in 1:nrow(data)) {
    # Get party preference and vote share data for the current respondent
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Exclude missing values (99)
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    
    # Find the most-liked party
    most_liked_party_index <- which.max(party_pref[valid_ratings])
    likemax <- party_pref[valid_ratings][most_liked_party_index]
    
    # Calculate the mean-distance score for the current respondent
    num_parties <- length(valid_ratings)
    likemax_square <- likemax^2
    sum_distance <- sum((party_pref[valid_ratings] - likemax)^2 * vote_share[valid_ratings])
    mean_distance <- sqrt(sum_distance / num_parties)
    
    # Store the mean-distance score for the current respondent
    mean_distance_scores[i] <- mean_distance
  }
  
  # Return the mean-distance scores for each respondent
  return(mean_distance_scores)
}

mean_distance_scores <- calculate_mean_distance(selected_d)
print(mean_distance_scores)



```



$$
\begin{equation}
\text{Spread}_i = \sqrt{\frac{1}{n_p} \sum_{p=1}^{P} (\text{like}_{ip} - \bar{\text{like}}_i)^2}
\end{equation}
$$

```{r, Wagner 2021 Spread_i}
calculate_affective_polarization <- function(data) {
  # List of party preference variables (likeip)
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  # Initialize a vector to store affective polarization scores for each respondent
  affective_polarization_scores <- numeric(nrow(data))
  
  # Loop through each respondent
  for (i in 1:nrow(data)) {
    # Get party preference data for the current respondent
    party_pref <- unlist(data[i, party_pref_vars])
    
    # Exclude missing values (99)
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    
    # Calculate the weighted average party affect for the current respondent
    likei <- mean(party_pref[valid_ratings])
    
    # Calculate the spread for the current respondent
    spread_i <- sqrt(sum((party_pref[valid_ratings] - likei)^2) / length(valid_ratings))
    
    # Store the affective polarization score for the current respondent
    affective_polarization_scores[i] <- spread_i
  }
  
  # Return the affective polarization scores for each respondent
  return(affective_polarization_scores)
}

# Example usage:
# Assuming 'data' is your dataset containing party preferences
# Replace 'data' with the actual name of your dataset
affective_polarization_scores <- calculate_affective_polarization(selected_d)
print(affective_polarization_scores)

```


```{r AP with selected_d (not accounting for France + special cases)}
calculate_affective_polarization_party <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  
  return(affective_polarization_scores)
}



affective_polarization_party_scores <- calculate_affective_polarization_party(selected_d, selected_d$IMD1005)
print(affective_polarization_party_scores)



# Convert matrix to data frame
AP_party_df <- as.data.frame(affective_polarization_party_scores)

# Add row names as a column
#AP_party_df$Respondent <- rownames(AP_party_df)


AP_party_df <- AP_party_df %>% 
  rename(
    ID = V1, 
    A = V2,
    B = V3,
    C = V4,
    D = V5,
    E = V6,
    F = V7,
    G = V8,
    H = V9,
    I = V10
  )


# Reshape data frame into long format
AP_party_long <- pivot_longer(AP_party_df, cols = -ID, names_to = "party_to", values_to = "WAP")
```


```{r, experiment correct but without ID}
calculate_affective_polarization_party <- function(data) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars))
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing party preferences
    valid_parties <- party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  return(affective_polarization_scores)
}

# Example usage:
affective_polarization_party_scores <- calculate_affective_polarization_party(selected_d)
print(affective_polarization_party_scores)



# Convert matrix to data frame
AP_party_df <- as.data.frame(affective_polarization_party_scores)

# Add row names as a column
AP_party_df$Respondent <- rownames(AP_party_df)


AP_party_df <- AP_party_df %>% 
  rename(
    A = V1,
    B = V2,
    C = V3,
    D = V4,
    E = V5,
    F = V6,
    G = V7,
    H = V8,
    I = V9
  )


# Reshape data frame into long format
AP_party_long <- pivot_longer(AP_party_df, cols = -Respondent, names_to = "party_to", values_to = "WAP")

```


# Simulation


```{r simulation with years 20 countries}
# Step 1: Modify the Model
mlm_1 <- lmer(WAP ~ to_pfeml  + (1 + to_pfeml | country), data = merged_data)
summary(mlm_1)

# Step 2: Update the Simulation Process
nsim <- 10000

# Step 2.1: Get the regression coefficients and variance-covariance matrix
beta_hat <- fixef(mlm_1)
V_hat <- vcov(mlm_1)

# Step 2.2: Draw from the multivariate normal distribution
S <- mvrnorm(nsim, beta_hat, V_hat)

# Step 2.3: Add random effects for all countries
country_names <- unique(merged_data$country)
num_countries <- length(country_names)

# Step 3: Create Covariate Scenarios for Years
min_to_pfeml <- min(merged_data$to_pfeml, na.rm = TRUE)
max_to_pfeml <- max(merged_data$to_pfeml, na.rm = TRUE)
to_pfeml_seq <- seq(min_to_pfeml, max_to_pfeml, length.out = 100)

years <- seq(2000, 2020, by = 1)

# Step 4: Calculate Expected Values for Each Country
EV_list <- list()
ev_mean_list <- list()
ev_ci_list <- list()

for (i in 1:num_countries) {
  country <- country_names[i]
  
  EV_country <- matrix(NA, nrow = length(to_pfeml_seq), ncol = length(years))
  for (j in 1:length(years)) {
    year <- years[j]
    
    # Filter data for the specific country and year
    country_data <- merged_data[merged_data$country == country & merged_data$year == year, ]
    
    # Create scenario for the country and year
    scenario <- cbind(1, to_pfeml_seq, rep(year, length(to_pfeml_seq)))
    
    # Calculate expected values
    EV <- S %*% t(scenario)
    
    # Store the results
    EV_country[, j] <- apply(EV, 2, mean)
  }
  
  EV_list[[country]] <- EV_country
}

# Set up the plot layout with adjusted margins
par(mfrow = c(5, 4), mar = c(4, 4, 2, 1))  

for (i in 1:num_countries) {
  country <- country_names[i]
  EV_country <- EV_list[[country]]
  
  plot(NULL, xlim = range(to_pfeml_seq), ylim = range(unlist(EV_list)), type = "n",
       main = paste("Female share and WAP in", country),
       xlab = "Average female share",
       ylab = "Expected value of WAP")
  
  for (j in 1:length(years)) {
    lines(x = to_pfeml_seq, y = EV_country[, j], col = j, lwd = 1.5)
  }
}

# Reset plot layout to default
par(mfrow = c(1, 1))


```


```{r, only country without confidence interval }

# Step 1: Modify the Model
mlm_1 <- lmer(WAP ~ to_pfeml + (1 + to_pfeml | country), data = merged_data)
summary(mlm_1)

# Step 2: Update the Simulation Process
nsim <- 10000

# Step 2.1: Get the regression coefficients and variance-covariance matrix
beta_hat <- fixef(mlm_1)
V_hat <- vcov(mlm_1)

# Step 2.2: Draw from the multivariate normal distribution
S <- mvrnorm(nsim, beta_hat, V_hat)

# Step 2.3: Add random effects for all countries
country_names <- unique(merged_data$country)
num_countries <- length(country_names)

# Step 3: Create Covariate Scenarios for Countries
min_to_pfeml <- min(merged_data$to_pfeml, na.rm = TRUE)
max_to_pfeml <- max(merged_data$to_pfeml, na.rm = TRUE)
to_pfeml_seq <- seq(min_to_pfeml, max_to_pfeml, length.out = 100)

# Step 4: Calculate Expected Values for Each Country
EV_list <- list()
ev_mean_list <- list()
ev_ci_list <- list()

for (i in 1:num_countries) {
  country <- country_names[i]
  
  EV_country <- matrix(NA, nrow = length(to_pfeml_seq), ncol = 1)
  scenario <- cbind(1, to_pfeml_seq)
  
  for (j in 1:nsim) {
    # Add random effects for the country
    S_sample <- S[j, ]
    S_sample[1] <- S_sample[1] + ranef(mlm_1)$country[country, "(Intercept)"]
    S_sample[2] <- S_sample[2] + ranef(mlm_1)$country[country, "to_pfeml"]
    
    # Calculate expected values
    EV <- S_sample %*% t(scenario)
    
    # Store the results
    EV_country[, 1] <- EV_country[, 1] + EV[, 1]
  }
  
  EV_list[[country]] <- EV_country / nsim
}

# Step 5: Plot the Results for Each Country
par(mfrow = c(5, 4), mar = c(4, 4, 2, 1))  # Set up the plot layout

for (i in 1:num_countries) {
  country <- country_names[i]
  EV_country <- EV_list[[country]]
  
  plot(to_pfeml_seq, EV_country[, 1], type = "l",
       main = paste("Female share and WAP in", country),
       xlab = "Average female share",
       ylab = "Expected value of WAP")
}

# Reset plot layout to default
par(mfrow = c(1, 1))

```


```{r, simulation of country with CI}

# Step 1: Modify the Model
mlm_1 <- lmer(WAP ~ to_pfeml + (1 + to_pfeml | country), data = merged_data)
summary(mlm_1)

# Step 2: Update the Simulation Process
nsim <- 1000  # Adjust the number of simulations as needed

# Step 2.1: Get the regression coefficients and variance-covariance matrix
beta_hat <- fixef(mlm_1)
V_hat <- vcov(mlm_1)

# Step 2.2: Draw from the multivariate normal distribution
S <- mvrnorm(nsim, beta_hat, V_hat)

# Step 2.3: Add random effects for all countries
country_names <- unique(merged_data$country)
num_countries <- length(country_names)

# Step 3: Create Covariate Scenarios for Countries
scenario_list <- list()

for (i in 1:num_countries) {
  country <- country_names[i]
  
  # Filter data for the specific country
  country_data <- merged_data[merged_data$country == country, ]
  
  # Calculate min and max to_pfeml for the country
  min_to_pfeml_country <- min(country_data$to_pfeml, na.rm = TRUE)
  max_to_pfeml_country <- max(country_data$to_pfeml, na.rm = TRUE)
  
  # Create scenario for the country
  to_pfeml_seq_country <- seq(min_to_pfeml_country, max_to_pfeml_country, length.out = 100)
  scenario_country <- cbind(1, to_pfeml_seq_country)
  
  # Store the scenario
  scenario_list[[country]] <- scenario_country
}

# Step 4: Calculate Expected Values and Confidence Intervals for Each Country
EV_list <- list()
ci_list <- list()

for (i in 1:num_countries) {
  country <- country_names[i]
  scenario_country <- scenario_list[[country]]
  
  EV_country <- matrix(NA, nrow = length(scenario_country), ncol = nsim)
  
  for (j in 1:nsim) {
    # Add random effects for the country
    S_sample <- S[j, ]
    S_sample[1] <- S_sample[1] + ranef(mlm_1)$country[country, "(Intercept)"]
    S_sample[2] <- S_sample[2] + ranef(mlm_1)$country[country, "to_pfeml"]
    
    # Calculate expected values
    EV <- S_sample %*% t(scenario_country)
    
    
    # Store the results
    EV_country[, j] <- EV[, 1]
  }
  
  # Calculate confidence intervals
  ci <- apply(EV_country, 1, quantile, c(0.025, 0.975))
  
  # Store the results
  EV_list[[country]] <- rowMeans(EV_country)
  ci_list[[country]] <- ci
}

# Step 5: Plot the Results for Each Country
par(mfrow = c(5, 4), mar = c(4, 4, 2, 1))  # Adjust margin size

for (i in 1:num_countries) {
  country <- country_names[i]
  
  # Get country-specific scenario, EV, and CI
  scenario_country <- scenario_list[[country]]
  EV_country <- EV_list[[country]]
  ci_country <- ci_list[[country]]
  
  # Plot expected values and confidence intervals
  plot(scenario_country[, 2], EV_country, type = "l", col = "black",
       main = paste("Female share and WAP in", country),
       xlab = "Average female share",
       ylab = "Expected value of WAP")
  
  lines(scenario_country[, 2], ci_country[1, ], col = "black", lty = "dashed")
  lines(scenario_country[, 2], ci_country[2, ], col = "black", lty = "dashed")
}

# Reset plot layout to default
par(mfrow = c(1, 1))




```


```{r, simulation correct one yet}

# List of countries
countries <- c("Australia", "Austria", "Canada", "Switzerland", "Germany", "Denmark", 
               "Finland", "France", "Great Britain", "Greece", "Ireland", "Iceland", 
               "Israel", "Netherlands", "New Zealand", "Norway", "Portugal", "Spain", 
               "Sweden", "United States of America") 

# Number of simulations
nsim <- 10000

# Step 1: Get the regression coefficients
# NOTE: We are only using fixed effects
beta_hat <- fixef(mlm_1)

# Step 2: Generate sampling distribution
# Step 2.1: Get the variance-covariance matrix
V_hat <- vcov(mlm_1)

# Step 2.2: Draw from the multivariate normal distribution
S <- mvrnorm(nsim, beta_hat, V_hat)

# Function to plot for a given country
plot_country <- function(country) {
  # Add random effects for the country
  S_country <- S
  S_country[, "(Intercept)"] <- S_country[, "(Intercept)"] + ranef(mlm_1)$country[country, "(Intercept)"]
  S_country[, "to_pfeml"] <- S_country[, "to_pfeml"] + ranef(mlm_1)$country[country, "to_pfeml"]
  
  # Get the range of 'to_pfeml' for the country
  min_to_pfeml <- min(merged_data$to_pfeml[merged_data$country == country], na.rm = TRUE)
  max_to_pfeml <- max(merged_data$to_pfeml[merged_data$country == country], na.rm = TRUE)
  to_pfeml_seq <- seq(min_to_pfeml, max_to_pfeml, length.out = 100)
  
  
  # Create the scenario matrix
  scenario <- cbind(1, to_pfeml_seq)
  
  # Calculate Quantities of Interest - Expected Values
  EV <- S_country %*% t(scenario)
  
  # Summarize
  ev_mean <- apply(EV, 2, mean)
  ev_ci <- apply(EV, 2, quantile, c(0.025, 0.975))
  
  # Plot
  plot(x = to_pfeml_seq,
       y = ev_mean,
       ylim = c(0.1, 5),
       xlim = c(0.0, 0.8),
       type = "l",
       lwd = 1.5,
       main = paste("Female share and WAP in", country),
       font.main = 1,
       ylab = "Expected value of WAP",
       xlab = "Average female share",
       las = 1)
  lines(x = to_pfeml_seq,
        y = ev_ci["2.5%", ],
        lty = "dashed",
        lwd = 1.5)
  lines(x = to_pfeml_seq,
        y = ev_ci["97.5%", ],
        lty = "dashed",
        lwd = 1.5)
}

# Loop through each country and plot
par(mfrow = c(5, 4), mar = c(5, 5, 2, 1))# Adjust layout to fit all plots in one view
for (country in countries) {
  plot_country(country)
}

```




# Plots 
```{r, plot data usisng heatmap (not a good idea tbh)}


# Convert matrix to data frame
#weighted_AP_df <- as.data.frame(weighted_AP_dyadic)

# Add row names as a column
#weighted_AP_df$Respondent <- rownames(weighted_AP_df)

# Convert WAP to numeric
AP_party_long$WAP <- as.numeric(AP_party_long$WAP)

# Reshape data frame into long format
weighted_AP_long <- pivot_longer(weighted_AP_df, cols = -Respondent, names_to = "Party", values_to = "WAP")

# Generate heatmap
ggplot(data = AP_party_long, aes(x = party_to, y = Respondent, fill = WAP)) +
  geom_tile(color = "white") +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Dyadic Weighted AP Scores", x = "Party", y = "Respondent") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))


```


```{r, try to plot the data unsing KDE}
ggplot(data = AP_party_long, aes(x = WAP)) +
  geom_density(fill = "skyblue", color = "blue", alpha = 0.5) +
  labs(title = "Kernel Density Estimation (KDE) Plot of Dyadic Weighted AP Scores", x = "WAP Score", y = "Density") +
  xlim(0, 3)+
  theme_minimal()
```


# Functions (maybe helpful)
```{r}
print_values_for_condition <- function(data, condition_col, condition_val, target_col) {
  # Filter the data based on the condition
  filtered_data <- data[data[[condition_col]] == condition_val, ]
  
  # Print the values of the target column for the filtered rows
  if (nrow(filtered_data) > 0) {
    print(filtered_data[[target_col]])
  } else {
    print(paste("No rows found where", condition_col, "is equal to", condition_val))
  }
}


# Print the values of IMD5001_A for rows where IMD1004 is "France2002"
print_values_for_condition(selected_d, "IMD1004", "FRA_2002", "IMD5005_I")

print_values_for_condition(selected_d, "IMD1004", "FRA_2012", "IMD5001_A")

print_values_for_condition(merged_data, "cntryyr", "Spain2008", "to_pfeml")
```

```{r simulation nur truncated ohne priors }
#library(dplyr)
#library(brms)
#library(MASS)  


rm(q)

# Filter data to include only the countries with the highest and lowest to_pfeml share
highest_lowest_countries <- merged_data %>%
  group_by(cntryyr) %>%
  summarise(avg_to_pfeml = mean(to_pfeml, na.rm = TRUE)) %>%
  top_n(2, avg_to_pfeml) %>%
  pull(cntryyr)

filtered_data <- merged_data %>%
  filter(cntryyr %in% highest_lowest_countries)

# Fit the model (if not already fitted)
# truncated_model <- brm(
#   bf(WAP | trunc(lb = 0, ub = 4) ~ to_pfeml + (1 + to_pfeml | cntryyr)),
#   data = filtered_data,
#   family = gaussian(),
#   chains = 4,
#   iter = 2000
# )

# Step 1: Get the regression coefficients (fixed effects)
beta_hat <- fixef(truncated_model)
#dim(beta_hat)

# Step 2.1: Get the variance-covariance matrix
V_hat <- vcov(truncated_model)

# Step 2.2: Draw samples from the multivariate normal distribution
nsim <- 10000
S <- mvrnorm(nsim, beta_hat, V_hat)

# Step 3: Add random effects
random_effects <- ranef(truncated_model)$cntryyr[highest_lowest_countries, , drop = FALSE]
S[, "(Intercept)"] <- S[, "(Intercept)"] + random_effects[, "(Intercept)"]
S[, "to_pfeml"] <- S[, "to_pfeml"] + random_effects[, "to_pfeml"]

# Step 4: Choose interesting covariate values
min_to_pfeml <- min(filtered_data$to_pfeml, na.rm = TRUE)
max_to_pfeml <- max(filtered_data$to_pfeml, na.rm = TRUE)
to_pfeml_seq <- seq(min_to_pfeml, max_to_pfeml, length.out = 100)
scenario <- cbind(1, to_pfeml_seq)

# Step 5: Calculate Quantities of Interest - Expected Values
EV <- S %*% t(scenario)

# Step 6: Summarize
ev_mean <- apply(EV, 2, mean)
ev_ci <- apply(EV, 2, quantile, c(0.025, 0.975))



```



```{r, qq-plot}
qqnorm(resid(mlm_1)) ; qqline(resid(mlm_1), col="red")
```


```{r, heteroskedastiscity}
ggplot(data = merged_data, aes(x = predict(mlm_1), y = cbind(resid(mlm_1)))) + 
  geom_point(size = 0.5, col = "red", alpha = 0.3) +
  geom_abline(slope = 0,
              intercept = 0,
              col = "gray") + 
  labs(x = "Fitted Values", y = "Residuals", 
       title = "Residuals vs Fitted", 
       subtitle = "Homogeneity") + 
  coord_fixed() 




# Fit the multilevel model
mlm_1 <- lmer(WAP ~ to_pfeml + (1 + to_pfeml | cntryyr), data = merged_data)

# Extract fitted values and residuals
merged_data$fitted_values <- fitted(mlm_1)
merged_data$residuals <- resid(mlm_1)

# Plot residuals vs. fitted values within each group
ggplot(merged_data, aes(x = fitted_values, y = residuals)) +
  geom_point(size = 2, color = "red", alpha = 0.5) +
  #facet_wrap(~ cntryyr) +  # Facet by group
  geom_hline(yintercept = 0, color = "gray", linetype = "dashed") +
  labs(x = "Fitted Values", y = "Residuals",
       title = "Residuals vs Fitted Values by Group",
       subtitle = "Assessing Within-Group Heteroscedasticity") +
  theme_minimal()

```


```{r, residuals mlm_1}
hist(resid(mlm_1), main = "Histogram of Residuals (normality)", 
     xlab = "Residuals", col = "red")
```


```{r, simulation mlm (working)}
nsim <- 10000

# Step 1: Get the regression coefficients
#   NOTE: We are only using fixed effects
beta_hat <- fixef(mlm_1)

# Step 2: Generate sampling distribution

# Step 2.1: Get the variance-covariance matrix.
V_hat <-  vcov(mlm_1)

# Step 2.2: Draw from the multivariate normal distribution.
S <- mvrnorm(nsim, beta_hat, V_hat)

# Additional Step 2.3: Add random effects

# varying intercept
S[, "(Intercept)"] <- 
  S[, "(Intercept)"] + 
  ranef(mlm_1)$country["Spain", "(Intercept)"]
# varying slope
S[, "to_pfeml"] <- 
  S[, "to_pfeml"] + 
  ranef(mlm_1)$country["Spain", "to_pfeml"]



# Step 3: Choose interesting covariate values. 
# Make sure the matrix multiplication also works for single scenarios
min_to_pfeml <- min(merged_data$to_pfeml[merged_data$country == "Spain"], na.rm = T)
max_to_pfeml <- max(merged_data$to_pfeml[merged_data$country == "Spain"], na.rm = T)
to_pfeml_seq <- seq(min_to_pfeml, max_to_pfeml,
                     length.out = 100)

scenario <- cbind(1, to_pfeml_seq)

# Step 4: Calculate Quantities of Interest - 
# Expected Values
EV <- S %*% t(scenario)

# Step 5: Summarize
ev_mean <- apply(EV, 2, mean)
ev_ci <- apply(EV, 2, quantile, c(0.025, 0.975))
```

```{r, plot simulation}
plot(x = to_pfeml_seq,
     y = ev_mean,
     ylim = c(0.1, 1),
     xlim = c(0.0, 0.5),
     type = "l",
     lwd = 1.5,
     main = "Female share and WAP in Spain",
     font.main = 1,
     ylab = "Expected value of WAP",
     xlab = "Average female share",
     las = 1)
lines(x = to_pfeml_seq,
      y = ev_ci["2.5%",],
      lty = "dashed",
      lwd = 1.5)
lines(x = to_pfeml_seq,
      y = ev_ci["97.5%",],
      lty = "dashed",
      lwd = 1.5)
```



```{r}

# Fit the model
mlm_1 <- lmer(WAP ~ to_pfeml + (1 + to_pfeml | cntryyr), data = merged_data)

# Extract variance components
var_comp <- as.data.frame(VarCorr(mlm_1))

# Extract the variance of the intercept for 'cntryyr'
var_cntryyr <- var_comp[var_comp$grp == "cntryyr" & var_comp$var1 == "(Intercept)" & is.na(var_comp$var2), "vcov"]

# Extract residual variance
var_residual <- attr(VarCorr(mlm_1), "sc")^2

# Calculate ICC
ICC <- var_cntryyr / (var_cntryyr + var_residual)
print(ICC)
```

```{r, perform first multilevel analysis}

# Adams et al. 2023 
#table1.1 <-lm(party_like ~ to_pfeml + as.factor(cntryyr), data = merged_data)
#summary(table1.1)


#table1 <- lm(WAP ~ to_pfeml + as.factor(cntryyr), data = merged_data)
#summary(table1)


mlm_1 <- lmer(WAP ~ to_pfeml +
                (1 + to_pfeml | cntryyr),
              data = merged_data)

summary(mlm_1)

ranef(mlm_1)




#table2 <- lm(WAP ~ to_pfeml + as.factor(country), data = merged_data)
#summary(table2)

#table3 <- lm(WAP ~ to_pfeml + as.factor(year), data = merged_data)
#summary(table3)


#load("raw-data/dyadic_data_1-4-22.Rdata")
```

```{r}
hist(merged_data[merged_data$cntryyr == "Australia1996", "WAP"])
```

```{r, nicht hilfreich}

# Extracting model coefficients and variance-covariance matrix
beta_hat <- fixef(truncated_model_with_priors)
V_hat <- vcov(truncated_model_with_priors)

# Define the number of simulations
nsim <- 10000

# Simulate coefficients
S <- mvrnorm(nsim, beta_hat[, 1], V_hat)

# Define a typical scenario (e.g., average country/year)
typical_random_effects <- c(Intercept = 0, to_pfeml = 0)  # Typical country/year has zero random effect
S[, "Intercept"] <- S[, "Intercept"] + typical_random_effects["Intercept"]
S[, "to_pfeml"] <- S[, "to_pfeml"] + typical_random_effects["to_pfeml"]

# Define a sequence of to_pfeml values
to_pfeml_seq <- seq(min(merged_data$to_pfeml, na.rm = TRUE), max(merged_data$to_pfeml, na.rm = TRUE), length.out = 100)
scenario <- cbind(1, to_pfeml_seq)

# Calculate expected values on the log scale
EV_log <- S %*% t(scenario)

# Exponentiate to get back to the original scale
EV <- exp(EV_log)

# Summarize the expected values
ev_mean <- apply(EV, 2, mean)
ev_ci <- apply(EV, 2, quantile, c(0.025, 0.975))

# Plot the expected values
plot(x = to_pfeml_seq,
     y = ev_mean,
     type = "l",
     lwd = 1.5,
     main = "Expected WAP as a function of Female Delegates Proportion",
     ylab = "Expected WAP",
     xlab = "Female Delegates Proportion",
     las = 1)
lines(x = to_pfeml_seq,
      y = ev_ci[1, ],
      lty = "dashed",
      lwd = 1.5)
lines(x = to_pfeml_seq,
      y = ev_ci[2, ],
      lty = "dashed",
      lwd = 1.5)


plot(x = to_pfeml_seq,
     y = ev_mean,
     type = "l",
     lwd = 1.5,
     main = "Expected WAP as a function of Female Delegates Proportion",
     ylab = "Expected WAP",
     xlab = "Female Delegates Proportion",
     las = 1,
     ylim = c(min(ev_ci), max(ev_ci)),  # Set ylim based on the CI
     xlim = c(min(to_pfeml_seq), max(to_pfeml_seq)))  # Set xlim based on the to_pfeml_seq range

lines(x = to_pfeml_seq,
      y = ev_ci[1, ],
      lty = "dashed",
      lwd = 1.5)
lines(x = to_pfeml_seq,
      y = ev_ci[2, ],
      lty = "dashed",
      lwd = 1.5)


```


```{r, fd for truncated model with priors}

# Filter data to include only the countries with the highest and lowest to_pfeml share
highest_lowest_countries <- merged_data %>%
  group_by(cntryyr) %>%
  summarise(avg_to_pfeml = mean(to_pfeml, na.rm = TRUE)) %>%
  arrange(desc(avg_to_pfeml)) %>%
  slice(c(1, n())) %>%
  pull(cntryyr)

filtered_data <- merged_data %>%
  filter(cntryyr %in% highest_lowest_countries)

# Step 1: Get the regression coefficients (fixed effects)
beta_hat <- fixef(truncated_model_with_priors)[, 1]  # Extract the fixed effects estimates

# Step 2.1: Get the variance-covariance matrix
V_hat <- vcov(truncated_model_with_priors)

# Step 2.2: Draw samples from the multivariate normal distribution
nsim <- 10000
S <- mvrnorm(nsim, beta_hat, V_hat)

# Step 3: Add random effects for the highest and lowest `to_pfeml` share countries
random_effects <- ranef(truncated_model_with_priors)$cntryyr[highest_lowest_countries, , , drop = FALSE]
random_effects_array <- array(0, dim = c(nsim, length(highest_lowest_countries), length(beta_hat)))

for (i in seq_along(highest_lowest_countries)) {
  country_re <- random_effects[highest_lowest_countries[i], 1, ]  # Extract random effects for each country
  random_effects_array[, i, ] <- mvrnorm(nsim, country_re, V_hat)
}

# Step 4: Choose interesting covariate values
min_to_pfeml <- min(filtered_data$to_pfeml, na.rm = TRUE)
max_to_pfeml <- max(filtered_data$to_pfeml, na.rm = TRUE)
to_pfeml_seq <- seq(min_to_pfeml, max_to_pfeml, length.out = 100)
scenario <- cbind(1, to_pfeml_seq)

# Step 5: Calculate Quantities of Interest - Expected Values for each country
EV <- array(0, dim = c(nsim, length(to_pfeml_seq), length(highest_lowest_countries)))
for (i in seq_along(highest_lowest_countries)) {
  EV[, , i] <- (S + random_effects_array[, i, ]) %*% t(scenario)
}

# Step 6: Calculate First Differences and summarize
fd <- EV[, , 1] - EV[, , 2]
fd_mean <- apply(fd, 2, mean)
fd_ci <- apply(fd, 2, quantile, c(0.025, 0.975))

# Back-transform from log scale to original scale
fd_mean_exp <- exp(fd_mean)
fd_ci_exp <- exp(fd_ci)

# Plotting
plot(x = to_pfeml_seq,
     y = fd_mean_exp,
     ylim = range(fd_ci_exp),
     type = "l",
     lwd = 1.5,
     main = "First Differences in Weighted Affective Polarization",
     font.main = 1,
     ylab = "Expected value of WAP",
     xlab = "Female Delegates Proportion",
     las = 1)
lines(x = to_pfeml_seq,
      y = fd_ci_exp[1, ],
      lty = "dashed",
      lwd = 1.5)
lines(x = to_pfeml_seq,
      y = fd_ci_exp[2, ],
      lty = "dashed",
      lwd = 1.5)

```



```{r, ja ber nur ein to_pfeml}
# Step 1: Get the regression coefficients (fixed effects)
beta_hat <- fixef(truncated_model_with_priors)
beta_hat <- beta_hat[, 1]

# Step 2.1: Get the variance-covariance matrix
V_hat <- vcov(truncated_model_with_priors)

# Germany 2017 (lowest/highest proportion)
nsim <- 10000
S_deu <- mvrnorm(nsim, beta_hat, V_hat)
random_effects_deu <- ranef(truncated_model_with_priors)$cntryyr["Germany2017", 1, ]
S_deu[, "Intercept"] <- S_deu[, "Intercept"] + random_effects_deu["Intercept"]
S_deu[, "to_pfeml"] <- S_deu[, "to_pfeml"] + random_effects_deu["to_pfeml"]

# Sweden (lowest/highest female proportion)
S_swe <- mvrnorm(nsim, beta_hat, V_hat)
random_effects_swe <- ranef(truncated_model_with_priors)$cntryyr["Sweden2006", 1, ]
S_swe[, "Intercept"] <- S_swe[, "Intercept"] + random_effects_swe["Intercept"]
S_swe[, "to_pfeml"] <- S_swe[, "to_pfeml"] + random_effects_swe["to_pfeml"]

# Step 4: Choose interesting covariate values
min_to_pfeml <- min(filtered_data$to_pfeml, na.rm = TRUE)
max_to_pfeml <- max(filtered_data$to_pfeml, na.rm = TRUE)
to_pfeml_seq <- seq(min_to_pfeml, max_to_pfeml, length.out = 100)
scenario <- cbind(1, to_pfeml_seq)

# Step 5: Calculate Quantities of Interest - Expected Values
EV_deu <- S_deu %*% t(scenario)
EV_swe <- S_swe %*% t(scenario)

# Exponentiate the predicted values and confidence intervals to get them back on the original scale
EV_deu <- exp(EV_deu)
EV_swe <- exp(EV_swe)

# Step 6: Summarize and plot
ev_mean_deu <- apply(EV_deu, 2, mean)
ev_ci_deu <- apply(EV_deu, 2, quantile, c(0.025, 0.975))

ev_mean_swe <- apply(EV_swe, 2, mean)
ev_ci_swe <- apply(EV_swe, 2, quantile, c(0.025, 0.975))

# Plot for Germany
plot(x = to_pfeml_seq,
     y = ev_mean_deu,
     ylim = range(c(ev_ci_deu, ev_ci_swe)),
     type = "l",
     lwd = 1.5,
     main = "Expected WAP in Germany",
     font.main = 1,
     ylab = "Expected WAP",
     xlab = "Female Delegates Proportion",
     las = 1)
lines(x = to_pfeml_seq,
      y = ev_ci_deu["2.5%", ],
      lty = "dashed",
      lwd = 1.5)
lines(x = to_pfeml_seq,
      y = ev_ci_deu["97.5%", ],
      lty = "dashed",
      lwd = 1.5)

# Plot for Sweden
plot(x = to_pfeml_seq,
     y = ev_mean_swe,
     ylim = range(c(ev_ci_deu, ev_ci_swe)),
     type = "l",
     lwd = 1.5,
     main = "Expected WAP in Sweden",
     font.main = 1,
     ylab = "Expected WAP",
     xlab = "Female Delegates Proportion",
     las = 1)
lines(x = to_pfeml_seq,
      y = ev_ci_swe["2.5%", ],
      lty = "dashed",
      lwd = 1.5)
lines(x = to_pfeml_seq,
      y = ev_ci_swe["97.5%", ],
      lty = "dashed",
      lwd = 1.5)

# Calculate First Differences
fd <- EV_swe - EV_deu

# Plot First Differences
plot(x = to_pfeml_seq,
     y = apply(fd, 2, mean),
     ylim = range(c(apply(fd, 2, quantile, 0.025), apply(fd, 2, quantile, 0.975))),
     type = "l",
     lwd = 1.5,
     main = "First Differences in Expected WAP",
     font.main = 1,
     ylab = "First Differences in Expected WAP",
     xlab = "Female Delegates Proportion",
     las = 1)
lines(x = to_pfeml_seq,
      y = apply(fd, 2, quantile, 0.025),
      lty = "dashed",
      lwd = 1.5)
lines(x = to_pfeml_seq,
      y = apply(fd, 2, quantile, 0.975),
      lty = "dashed",
      lwd = 1.5)

```



```{r, ja aber bring usn nicht weiter hier }

# Filter data to include only the countries with the highest and lowest to_pfeml share
highest_lowest_countries <- merged_data %>%
  group_by(cntryyr) %>%
  summarise(avg_to_pfeml = mean(to_pfeml, na.rm = TRUE)) %>%
  arrange(desc(avg_to_pfeml)) %>%
  slice(c(1, n())) %>%
  pull(cntryyr)

filtered_data <- merged_data %>%
  filter(cntryyr %in% highest_lowest_countries)
# Extract relevant data for Germany and Australia
data_germany <- filtered_data %>% filter(cntryyr == "Germany2017")
data_australia <- filtered_data %>% filter(cntryyr == "Australia1996")

# Step 1: Get the regression coefficients (fixed effects)
beta_hat <- fixef(truncated_model_with_priors)
beta_hat <- beta_hat[, 1]

# Step 2.1: Get the variance-covariance matrix
V_hat <- vcov(truncated_model_with_priors)

# Germany 2017 (low proportion of female delegates)
nsim <- 10000
S_deu <- mvrnorm(nsim, beta_hat, V_hat)
random_effects_deu <- ranef(truncated_model_with_priors)$cntryyr["Germany2017", 1, ]
S_deu[, "Intercept"] <- S_deu[, "Intercept"] + random_effects_deu["Intercept"]
S_deu[, "to_pfeml"] <- S_deu[, "to_pfeml"] + random_effects_deu["to_pfeml"]

# Australia 1996 (high proportion of female delegates)
S_aus <- mvrnorm(nsim, beta_hat, V_hat)
random_effects_aus <- ranef(truncated_model_with_priors)$cntryyr["Australia1996", 1, ]
S_aus[, "Intercept"] <- S_aus[, "Intercept"] + random_effects_aus["Intercept"]
S_aus[, "to_pfeml"] <- S_aus[, "to_pfeml"] + random_effects_aus["to_pfeml"]

# Step 4: Choose covariate values specific to each country
min_to_pfeml_deu <- min(data_germany$to_pfeml, na.rm = TRUE)
max_to_pfeml_deu <- max(data_germany$to_pfeml, na.rm = TRUE)
to_pfeml_seq_deu <- seq(min_to_pfeml_deu, max_to_pfeml_deu, length.out = 100)
scenario_deu <- cbind(1, to_pfeml_seq_deu)

min_to_pfeml_aus <- min(data_australia$to_pfeml, na.rm = TRUE)
max_to_pfeml_aus <- max(data_australia$to_pfeml, na.rm = TRUE)
to_pfeml_seq_aus <- seq(min_to_pfeml_aus, max_to_pfeml_aus, length.out = 100)
scenario_aus <- cbind(1, to_pfeml_seq_aus)

# Step 5: Calculate Quantities of Interest - Expected Values
EV_deu <- S_deu %*% t(scenario_deu)
EV_aus <- S_aus %*% t(scenario_aus)

# Exponentiate the predicted values and confidence intervals to get them back on the original scale
EV_deu <- exp(EV_deu)
EV_aus <- exp(EV_aus)

# Step 6: Summarize and plot
ev_mean_deu <- apply(EV_deu, 2, mean)
ev_ci_deu <- apply(EV_deu, 2, quantile, c(0.025, 0.975))

ev_mean_aus <- apply(EV_aus, 2, mean)
ev_ci_aus <- apply(EV_aus, 2, quantile, c(0.025, 0.975))

# Plot for Germany
plot(x = to_pfeml_seq_deu,
     y = ev_mean_deu,
     ylim = range(c(ev_ci_deu, ev_ci_aus)),
     type = "l",
     lwd = 1.5,
     main = "Expected WAP in Germany 2017",
     font.main = 1,
     ylab = "Expected WAP",
     xlab = "Female Delegates Proportion",
     las = 1)
lines(x = to_pfeml_seq_deu,
      y = ev_ci_deu["2.5%", ],
      lty = "dashed",
      lwd = 1.5)
lines(x = to_pfeml_seq_deu,
      y = ev_ci_deu["97.5%", ],
      lty = "dashed",
      lwd = 1.5)

# Plot for Australia
plot(x = to_pfeml_seq_aus,
     y = ev_mean_aus,
     ylim = range(c(ev_ci_deu, ev_ci_aus)),
     type = "l",
     lwd = 1.5,
     main = "Expected WAP in Australia 1996",
     font.main = 1,
     ylab = "Expected WAP",
     xlab = "Female Delegates Proportion",
     las = 1)
lines(x = to_pfeml_seq_aus,
      y = ev_ci_aus["2.5%", ],
      lty = "dashed",
      lwd = 1.5)
lines(x = to_pfeml_seq_aus,
      y = ev_ci_aus["97.5%", ],
      lty = "dashed",
      lwd = 1.5)

# Calculate First Differences
min_length <- min(length(to_pfeml_seq_deu), length(to_pfeml_seq_aus))
fd <- EV_aus[, 1:min_length] - EV_deu[, 1:min_length]


# Plot First Differences
plot(x = to_pfeml_seq_deu[1:min_length],
     y = apply(fd, 2, mean),
     ylim = range(c(apply(fd, 2, quantile, 0.025), apply(fd, 2, quantile, 0.975))),
     type = "l",
     lwd = 1.5,
     main = "First Differences in Expected WAP",
     font.main = 1,
     ylab = "First Differences in Expected WAP",
     xlab = "Female Delegates Proportion",
     las = 1)
lines(x = to_pfeml_seq_deu[1:min_length],
      y = apply(fd, 2, quantile, 0.025),
      lty = "dashed",
      lwd = 1.5)
lines(x = to_pfeml_seq_deu[1:min_length],
      y = apply(fd, 2, quantile, 0.975),
      lty = "dashed",
      lwd = 1.5)



```









```{r, spread like Wagner}
calculate_weighted_AP <- function(data) {
  # List of party preference variables (IMD3008_A to IMD3008_I)
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  # List of vote share variables (IMD5001_A to IMD5001_I)
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  # Initialize a vector to store weighted AP scores for each respondent
  weighted_AP_scores <- numeric(nrow(data))
  
  # Loop through each respondent
  for (i in 1:nrow(data)) {
    # Get party preference and vote share data for the current respondent
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars])
    
    # Exclude missing values (99)
    valid_ratings <- which(!is.na(party_pref) & party_pref != 99)
    
    # Calculate the weighted average party affect for the current respondent
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the spread for the current respondent
    spread_i <- sqrt(sum((party_pref[valid_ratings] - likei)^2 * vote_share[valid_ratings]))
    
    # Store the weighted AP score for the current respondent
    weighted_AP_scores[i] <- spread_i
  }
  
  # Return the weighted AP scores for each respondent
  return(weighted_AP_scores)
}


weighted_AP_scores <- calculate_weighted_AP(selected_d)
print(weighted_AP_scores)

```


```{r}
calculate_affective_polarization_party <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref >= 0 & party_pref <= 10 & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing and valid party preferences
    valid_parties <- party_pref >= 0 & party_pref <= 10 & party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  return(affective_polarization_scores)
}


AP_Full_scores <- calculate_affective_polarization_party(Full_selected_d, Full_selected_d$IMD1005)
print(AP_Full_scores)

```
```{r}
calculate_AP_FRA <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5005_A", "IMD5005_B", "IMD5005_C", "IMD5005_D", 
                       "IMD5005_E", "IMD5005_F", "IMD5005_G", "IMD5005_H", "IMD5005_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref >= 0 & party_pref <= 10 & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing and valid party preferences
    valid_parties <- party_pref >= 0 & party_pref <= 10 & party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  return(affective_polarization_scores)
}

AP_FRA_scores <- calculate_AP_FRA(FRA_selected_d, FRA_selected_d$IMD1005)
print(AP_FRA_scores)

```

```{r}
calculate_affective_polarization_party <- function(data, respondent_ids) {
  party_pref_vars <- c("IMD3008_A", "IMD3008_B", "IMD3008_C", "IMD3008_D", 
                       "IMD3008_E", "IMD3008_F", "IMD3008_G", "IMD3008_H", "IMD3008_I")
  
  vote_share_vars <- c("IMD5001_A", "IMD5001_B", "IMD5001_C", "IMD5001_D", 
                       "IMD5001_E", "IMD5001_F", "IMD5001_G", "IMD5001_H", "IMD5001_I")
  
  affective_polarization_scores <- matrix(0, nrow = nrow(data), ncol = length(party_pref_vars) + 1)  # +1 for the respondent ID
  
  for (i in 1:nrow(data)) {
    party_pref <- unlist(data[i, party_pref_vars])
    vote_share <- unlist(data[i, vote_share_vars]) 
    
    # Initialize affective polarization scores for the current respondent
    affective_polarization_row <- rep(0, length(party_pref))
    
    # Calculate the average like-dislike score for the current respondent
    valid_ratings <- which(!is.na(party_pref) & party_pref >= 0 & party_pref <= 10 & party_pref != 99)
    likei <- sum(party_pref[valid_ratings] * vote_share[valid_ratings])
    
    # Calculate the affective polarization score for non-missing and valid party preferences
    valid_parties <- party_pref >= 0 & party_pref <= 10 & party_pref != 99
    affective_polarization_row[valid_parties] <- sqrt(vote_share[valid_parties] * (party_pref[valid_parties] - likei)^2)
    
    # Add respondent ID to the beginning of the row
    affective_polarization_row <- c(respondent_ids[i], affective_polarization_row)
    
    # Store the affective polarization scores for the current respondent
    affective_polarization_scores[i, ] <- affective_polarization_row
  }
  
  return(affective_polarization_scores)
}

AP_six_spe <- calculate_affective_polarization_party(combined_df, combined_df$IMD1005)
print(AP_six_spe)

```
